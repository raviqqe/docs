{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to CAST AI \u00b6 It is one platform that cuts your cloud costs, boosts DevOps, and prevents downtime. Easy to start \u00b6 Deploy your first cluster in 5-10 minutes. Follow our getting started guide to try it out! Or connect your cluster with read-only mode and we will calculate and provide you estimated monthly savings. Community \u00b6 We invite you to join our community and find out what is trending in the atmosphere; connect with our engineers and support, monitor release-notes, explore technical discussions, and more: Join our Discord channel Join our Slack channel Events \u00b6 The CAST AI team loves sharing knowledge and experience in webinars! If you would like to learn more about our journey and have your questions answered by our CTO and engineers LIVE - please check out our events calendar . Support \u00b6 Feel free to ask any questions on slack or discord. You can also reach our support on the website chat or email: support@cast.ai","title":"Introduction"},{"location":"#welcome-to-cast-ai","text":"It is one platform that cuts your cloud costs, boosts DevOps, and prevents downtime.","title":"Welcome to CAST AI"},{"location":"#easy-to-start","text":"Deploy your first cluster in 5-10 minutes. Follow our getting started guide to try it out! Or connect your cluster with read-only mode and we will calculate and provide you estimated monthly savings.","title":"Easy to start"},{"location":"#community","text":"We invite you to join our community and find out what is trending in the atmosphere; connect with our engineers and support, monitor release-notes, explore technical discussions, and more: Join our Discord channel Join our Slack channel","title":"Community"},{"location":"#events","text":"The CAST AI team loves sharing knowledge and experience in webinars! If you would like to learn more about our journey and have your questions answered by our CTO and engineers LIVE - please check out our events calendar .","title":"Events"},{"location":"#support","text":"Feel free to ask any questions on slack or discord. You can also reach our support on the website chat or email: support@cast.ai","title":"Support"},{"location":"api/authentication/","text":"Authentication \u00b6 Before you can use our API, either with your preferred REST client or via Terraform, you will need an API key. Obtaining API access key \u00b6 From the top menu in the CAST AI console, open API | API access keys , select create access key and name your key: We advise using the descriptive name for your intended purpose - it will be easier to distinguish which key is used for which integration if you add more keys later. Important When the key is created - save it because you will not be able to view the key again after this window is closed . The reason API key value is visible only at the time of creation is that we do not store the key in plain text on our system. For security reasons, CAST AI \"forgets\" key value after giving it to you, and later is only able to verify if the key is valid, but not to re-retrieve the value for you. If you lose your key, the only solution is to create a new key. CAST AI Swagger setup \u00b6 You can test your key directly in our API specification . Visit https://api.cast.ai/v1/spec/ , click \"Authorize\" and enter your key for X-API-Key field. After setting this up, you are now ready to use the \"Try it out\" button that is available for each endpoint. Using keys in API calls \u00b6 To authenticate, provide the key in X-API-Key HTTP header. For example, for curl this would be: ``` curl -X GET \" https://api.cast.ai/v1/kubernetes/clusters \" -H \"X-API-Key: your-api-key-here\" | jq","title":"Authentication"},{"location":"api/authentication/#authentication","text":"Before you can use our API, either with your preferred REST client or via Terraform, you will need an API key.","title":"Authentication"},{"location":"api/authentication/#obtaining-api-access-key","text":"From the top menu in the CAST AI console, open API | API access keys , select create access key and name your key: We advise using the descriptive name for your intended purpose - it will be easier to distinguish which key is used for which integration if you add more keys later. Important When the key is created - save it because you will not be able to view the key again after this window is closed . The reason API key value is visible only at the time of creation is that we do not store the key in plain text on our system. For security reasons, CAST AI \"forgets\" key value after giving it to you, and later is only able to verify if the key is valid, but not to re-retrieve the value for you. If you lose your key, the only solution is to create a new key.","title":"Obtaining API access key"},{"location":"api/authentication/#cast-ai-swagger-setup","text":"You can test your key directly in our API specification . Visit https://api.cast.ai/v1/spec/ , click \"Authorize\" and enter your key for X-API-Key field. After setting this up, you are now ready to use the \"Try it out\" button that is available for each endpoint.","title":"CAST AI Swagger setup"},{"location":"api/authentication/#using-keys-in-api-calls","text":"To authenticate, provide the key in X-API-Key HTTP header. For example, for curl this would be: ``` curl -X GET \" https://api.cast.ai/v1/kubernetes/clusters \" -H \"X-API-Key: your-api-key-here\" | jq","title":"Using keys in API calls"},{"location":"api/cli/","text":"CLI \u00b6 Your CAST.AI infrastructure can be automated via CAST AI Command Line Interface (CLI). Installation steps, examples, and releases are available at the repository: https://github.com/castai/cli .","title":"CLI"},{"location":"api/cli/#cli","text":"Your CAST.AI infrastructure can be automated via CAST AI Command Line Interface (CLI). Installation steps, examples, and releases are available at the repository: https://github.com/castai/cli .","title":"CLI"},{"location":"api/overview/","text":"Accessing CAST AI services via API \u00b6 Overview \u00b6 We build our services at CAST AI API-first; anything you can do in our console UI is available via REST API. You can use either your preferred way to call REST services directly or leverage our Terraform plugin to automate your infrastructure provisioning.","title":"Introduction"},{"location":"api/overview/#accessing-cast-ai-services-via-api","text":"","title":"Accessing CAST AI services via API"},{"location":"api/overview/#overview","text":"We build our services at CAST AI API-first; anything you can do in our console UI is available via REST API. You can use either your preferred way to call REST services directly or leverage our Terraform plugin to automate your infrastructure provisioning.","title":"Overview"},{"location":"api/specification/","text":"API specification \u00b6 Our API contract is published as OpenAPI v3 specification. You can check it on our Swagger UI: https://api.cast.ai This will bring you to our current specification. Here you will be able to familiarize yourself with available APIs and try functionality directly in the browser. To try out APIs in the browser you will need an API access key. See Authentication . We do not maintain any public SDKs but you can generate an API client for your programming language using many of the OpenAPI generators . Use below JSON as a spec: https://api.cast.ai/v1/spec/openapi.json OpenAPI is widely supported. Many tools (e.g. Postman) allow importing OpenAPI definitions as well. See documentation for your REST tooling to find out more.","title":"Specification"},{"location":"api/specification/#api-specification","text":"Our API contract is published as OpenAPI v3 specification. You can check it on our Swagger UI: https://api.cast.ai This will bring you to our current specification. Here you will be able to familiarize yourself with available APIs and try functionality directly in the browser. To try out APIs in the browser you will need an API access key. See Authentication . We do not maintain any public SDKs but you can generate an API client for your programming language using many of the OpenAPI generators . Use below JSON as a spec: https://api.cast.ai/v1/spec/openapi.json OpenAPI is widely supported. Many tools (e.g. Postman) allow importing OpenAPI definitions as well. See documentation for your REST tooling to find out more.","title":"API specification"},{"location":"api/terraform-provider/","text":"Terraform provider \u00b6 Your CAST.AI infrastructure can be automated via Terraform using terraform-provider-castai provider. Installation steps, example projects, and releases are available at the repository: https://github.com/castai/terraform-provider-castai .","title":"Terraform provider"},{"location":"api/terraform-provider/#terraform-provider","text":"Your CAST.AI infrastructure can be automated via Terraform using terraform-provider-castai provider. Installation steps, example projects, and releases are available at the repository: https://github.com/castai/terraform-provider-castai .","title":"Terraform provider"},{"location":"concepts/cluster-architecture/","text":"Cluster architecture \u00b6 Context \u00b6 When you create a cluster you can download kubeconfig to access your cluster directly. Some of the middleware that is running on the cluster (Grafana, Kubernetes dashboard) is directly reachable from console UI through the single sign-on gateway. You can notice on the diagram below that there is a bi-direction link between your cluster and CAST AI platform. Not only the platform connects to your cloud infrastructure or the cluster itself; CAST AI also relies on the cluster to \"call back\" and inform about certain events: Cluster control plane nodes actions with provisioning engine, e.g. when to join the cluster; Nodes inform about operations being completed, like finishing joining the cluster; Relevant cloud events get propagated to provisioning engine & autoscaler, for example, \"spot instance is being terminated by cloud provider\"; Your app users do not interact with CAST AI in any way. You own your kubernetes cluster infrastructure 100%, including any ingress infrastructure to reach your cluster workloads. Below diagram highlights primary groups of components that define a relationship between CAST AI platform and your cluster:","title":"Cluster architecture"},{"location":"concepts/cluster-architecture/#cluster-architecture","text":"","title":"Cluster architecture"},{"location":"concepts/cluster-architecture/#context","text":"When you create a cluster you can download kubeconfig to access your cluster directly. Some of the middleware that is running on the cluster (Grafana, Kubernetes dashboard) is directly reachable from console UI through the single sign-on gateway. You can notice on the diagram below that there is a bi-direction link between your cluster and CAST AI platform. Not only the platform connects to your cloud infrastructure or the cluster itself; CAST AI also relies on the cluster to \"call back\" and inform about certain events: Cluster control plane nodes actions with provisioning engine, e.g. when to join the cluster; Nodes inform about operations being completed, like finishing joining the cluster; Relevant cloud events get propagated to provisioning engine & autoscaler, for example, \"spot instance is being terminated by cloud provider\"; Your app users do not interact with CAST AI in any way. You own your kubernetes cluster infrastructure 100%, including any ingress infrastructure to reach your cluster workloads. Below diagram highlights primary groups of components that define a relationship between CAST AI platform and your cluster:","title":"Context"},{"location":"concepts/cluster-infrastructure/","text":"Cluster infrastructure \u00b6 Nodes \u00b6 Overview on where cluster virtual machines will be provisioned on your cloud: Ingress \u00b6 CAST AI provisioned clusters contain all the infrastructure needed to equip your app with an external TLS endpoint: DNS entry to round-robin; Load-balancing infrastructure: cloud-native load balancers that route traffic to a sub-section of your cluster (e.g. traffic that hits AWS load balancer will route to AWS nodes); Nginx ingress controller, paired with TLS certificate manager, that listen to your deployed resources and maintain routing&TLS configuration; Metric collection for your ingress traffic; All that is left for you as an application developer is to deploy your app, ingress resource, and configure a domain alias of your choice. See the ingress guide for more details. Network details \u00b6 Region & zone \u00b6 Each cloud maps a selected CAST AI region to a matching region on that cloud. For example, US East (Ashburn) region maps to: AWS: us-east-1 GCP: us-east4 Azure: eastus Digital Ocean: nyc1 Currently, on each cloud CAST AI builds a single-zone setup of your cluster. Zone selection is cloud-specific. Master nodes inbound \u00b6 Protocol Port Source Description tcp 6443 0.0.0.0/0 k8s API server udp 51820 0.0.0.0/0 WireGuard (if used) Worker nodes inbound \u00b6 Protocol Port Source Description udp 51820 0.0.0.0/0 WireGuard (if used) tcp/udp NodePort 0.0.0.0/0 k8s Service with type=LoadBalancer Subnets \u00b6 Range Description 10.96.0.0/12 k8s services 10.217.0.0/16 k8s pods 10.4.0.0/16 WireGuard 10.0.0.0/16 GCP VPC. Smaller /24 blocks are allocated for subnets. 10.10.0.0/16 AWS VPC. Smaller /24 blocks are allocated for subnets. 10.20.0.0/16 AZURE VPC. Smaller /24 blocks are allocated for subnets. 10.100-255.0.0/20 DigitalOcean VPC. There is only one subnet that is allocated dynamically.","title":"Cluster infrastructure"},{"location":"concepts/cluster-infrastructure/#cluster-infrastructure","text":"","title":"Cluster infrastructure"},{"location":"concepts/cluster-infrastructure/#nodes","text":"Overview on where cluster virtual machines will be provisioned on your cloud:","title":"Nodes"},{"location":"concepts/cluster-infrastructure/#ingress","text":"CAST AI provisioned clusters contain all the infrastructure needed to equip your app with an external TLS endpoint: DNS entry to round-robin; Load-balancing infrastructure: cloud-native load balancers that route traffic to a sub-section of your cluster (e.g. traffic that hits AWS load balancer will route to AWS nodes); Nginx ingress controller, paired with TLS certificate manager, that listen to your deployed resources and maintain routing&TLS configuration; Metric collection for your ingress traffic; All that is left for you as an application developer is to deploy your app, ingress resource, and configure a domain alias of your choice. See the ingress guide for more details.","title":"Ingress"},{"location":"concepts/cluster-infrastructure/#network-details","text":"","title":"Network details"},{"location":"concepts/cluster-infrastructure/#region-zone","text":"Each cloud maps a selected CAST AI region to a matching region on that cloud. For example, US East (Ashburn) region maps to: AWS: us-east-1 GCP: us-east4 Azure: eastus Digital Ocean: nyc1 Currently, on each cloud CAST AI builds a single-zone setup of your cluster. Zone selection is cloud-specific.","title":"Region &amp; zone"},{"location":"concepts/cluster-infrastructure/#master-nodes-inbound","text":"Protocol Port Source Description tcp 6443 0.0.0.0/0 k8s API server udp 51820 0.0.0.0/0 WireGuard (if used)","title":"Master nodes inbound"},{"location":"concepts/cluster-infrastructure/#worker-nodes-inbound","text":"Protocol Port Source Description udp 51820 0.0.0.0/0 WireGuard (if used) tcp/udp NodePort 0.0.0.0/0 k8s Service with type=LoadBalancer","title":"Worker nodes inbound"},{"location":"concepts/cluster-infrastructure/#subnets","text":"Range Description 10.96.0.0/12 k8s services 10.217.0.0/16 k8s pods 10.4.0.0/16 WireGuard 10.0.0.0/16 GCP VPC. Smaller /24 blocks are allocated for subnets. 10.10.0.0/16 AWS VPC. Smaller /24 blocks are allocated for subnets. 10.20.0.0/16 AZURE VPC. Smaller /24 blocks are allocated for subnets. 10.100-255.0.0/20 DigitalOcean VPC. There is only one subnet that is allocated dynamically.","title":"Subnets"},{"location":"concepts/cluster-lifecycle/","text":"Cluster lifecycle \u00b6 1. Provisioning \u00b6 You initiate the creation of the cluster - see create cluster . 2. Reconciliation & healing \u00b6 A cluster enters a reconciliation loop. The platform periodically re-checks that actual infrastructure on your cloud reflects the specified configuration, and performs upgrades & patching. Reconciliation performs checks such as: Cluster network configuration is up to date; Are any nodes missing, e.g. accidentally deleted; Are there any unused resources to clean up; 3. Resizing \u00b6 CAST AI clusters do not use a \"node pool\" concept. Instead, you can: Manually add or remove nodes with the specified configuration. Enable autoscaling policies - it scales up and down per-node level. 4. Cleanup \u00b6 When you delete a cluster platform will collapse cloud resources in the quickest way. Nodes will not be drained before deleting them. The platform is designed to minimize unintended removals. If you have any extra virtual machines that do not contain CAST AI cluster UUID - the delete operation will fail.","title":"Cluster lifecycle"},{"location":"concepts/cluster-lifecycle/#cluster-lifecycle","text":"","title":"Cluster lifecycle"},{"location":"concepts/cluster-lifecycle/#1-provisioning","text":"You initiate the creation of the cluster - see create cluster .","title":"1. Provisioning"},{"location":"concepts/cluster-lifecycle/#2-reconciliation-healing","text":"A cluster enters a reconciliation loop. The platform periodically re-checks that actual infrastructure on your cloud reflects the specified configuration, and performs upgrades & patching. Reconciliation performs checks such as: Cluster network configuration is up to date; Are any nodes missing, e.g. accidentally deleted; Are there any unused resources to clean up;","title":"2. Reconciliation &amp; healing"},{"location":"concepts/cluster-lifecycle/#3-resizing","text":"CAST AI clusters do not use a \"node pool\" concept. Instead, you can: Manually add or remove nodes with the specified configuration. Enable autoscaling policies - it scales up and down per-node level.","title":"3. Resizing"},{"location":"concepts/cluster-lifecycle/#4-cleanup","text":"When you delete a cluster platform will collapse cloud resources in the quickest way. Nodes will not be drained before deleting them. The platform is designed to minimize unintended removals. If you have any extra virtual machines that do not contain CAST AI cluster UUID - the delete operation will fail.","title":"4. Cleanup"},{"location":"concepts/how-it-works/","text":"How it works \u00b6 The CAST AI engine uses your Cloud Service Provider (CSP) accounts to create the required cloud resources and set up a multi cloud cluster for you. Start using multi cloud Kubernetes with a few clicks . Multi cloud network \u00b6 CAST AI uses your owned and provided CSP accounts to create VPCs or Resource Groups (depending on the cloud services you use). CAST AI creates the required network (like subnets, public IPs, and VPNs) to ensure a uniform network across created VPCs for a seamless Kubernetes operation. Processes behind it help non-compatible clouds merge into a single flat network. CAST AI selects regions with network latency in mind. For your applications and cluster to function as expected, cross-cloud latency shouldn't go above 10 ms in normal operation. The CAST AI regions were measured to operate in a 5-7 ms range. Enter Kubernetes \u00b6 With the network in place: VMs are added to take the role of Kubernetes Masters and Workers. You can add or remove Worker nodes in the /nodes menu. Cluster enters a reconcilation loop . If you delete any resources from the provided CSP accounts manually, CAST AI recreates them to the specification set by you in the console. No instant changes to the cluster are allowed during the time of reconciliation. You can only apply them after the reconciliation. Automated cleanup \u00b6 When you delete a cluster via the CAST AI console , the operation will terminate all VMs and delete the cloud resources (attached storage, public IPs, VPN connections, network subnets, etc.). To further understand the lifecycle of a cluster, check our Cluster lifecycle overview.","title":"How it works"},{"location":"concepts/how-it-works/#how-it-works","text":"The CAST AI engine uses your Cloud Service Provider (CSP) accounts to create the required cloud resources and set up a multi cloud cluster for you. Start using multi cloud Kubernetes with a few clicks .","title":"How it works"},{"location":"concepts/how-it-works/#multi-cloud-network","text":"CAST AI uses your owned and provided CSP accounts to create VPCs or Resource Groups (depending on the cloud services you use). CAST AI creates the required network (like subnets, public IPs, and VPNs) to ensure a uniform network across created VPCs for a seamless Kubernetes operation. Processes behind it help non-compatible clouds merge into a single flat network. CAST AI selects regions with network latency in mind. For your applications and cluster to function as expected, cross-cloud latency shouldn't go above 10 ms in normal operation. The CAST AI regions were measured to operate in a 5-7 ms range.","title":"Multi cloud network"},{"location":"concepts/how-it-works/#enter-kubernetes","text":"With the network in place: VMs are added to take the role of Kubernetes Masters and Workers. You can add or remove Worker nodes in the /nodes menu. Cluster enters a reconcilation loop . If you delete any resources from the provided CSP accounts manually, CAST AI recreates them to the specification set by you in the console. No instant changes to the cluster are allowed during the time of reconciliation. You can only apply them after the reconciliation.","title":"Enter Kubernetes"},{"location":"concepts/how-it-works/#automated-cleanup","text":"When you delete a cluster via the CAST AI console , the operation will terminate all VMs and delete the cloud resources (attached storage, public IPs, VPN connections, network subnets, etc.). To further understand the lifecycle of a cluster, check our Cluster lifecycle overview.","title":"Automated cleanup"},{"location":"concepts/vpn-overview/","text":"Cluster VPN overview \u00b6 This chapter summarizes Virtual Private Network (VPN) architecture for multi-cloud private networks. Cloud provided VPN \u00b6 Cloud provided VPN - a VPN option provided by a cloud service provider. Cloud provided VPN is currently not available on Digital Ocean. Traffic in CAST AI platform: In order to access the Virtual Private Cloud (VPC) network of each node CAST AI platform provisions managed Highly Available (HA) VPN gateways. VPN Gateway is created on each cloud and adds additional cluster costs. Traffic between nodes in different VPC is always encrypted. Traffic between nodes in the same VPC is plaintext. Example with nodes on AWS, AZURE and GCP clouds: WireGuard VPN \u00b6 WireGuard VPN - CAST AI integrated alternative to Cloud provided VPN. This option is optimized for saving cost. You can choose between two topologies: Topology Description Full Mesh Traffic is encrypted between each node even if it is located in the same VPC. Cross Location Mesh Traffic is encrypted only between nodes in different VPC. Traffic in CAST AI platform: Each node runs a WireGuard kernel module. VPN peers configuration, keys exchange, network interfaces and routing tables are fully managed by the CAST AI platform. Each node has public IP for node-to-node communication. Firewall rules allow sending/receive UDP packets on 51820 port for each node. Each node is assigned a private IP from cloud's VPC subnet range and WireGuard interface IP from 10.4.0.0/16 subnet. Traffic between nodes in different VPC is always encrypted. Traffic between nodes in the same VPC is encrypted or plaintext depending on topology selection. Example with nodes on AWS, AZURE, GCP, and DIGITAL OCEAN clouds:","title":"Cluster VPN overview"},{"location":"concepts/vpn-overview/#cluster-vpn-overview","text":"This chapter summarizes Virtual Private Network (VPN) architecture for multi-cloud private networks.","title":"Cluster VPN overview"},{"location":"concepts/vpn-overview/#cloud-provided-vpn","text":"Cloud provided VPN - a VPN option provided by a cloud service provider. Cloud provided VPN is currently not available on Digital Ocean. Traffic in CAST AI platform: In order to access the Virtual Private Cloud (VPC) network of each node CAST AI platform provisions managed Highly Available (HA) VPN gateways. VPN Gateway is created on each cloud and adds additional cluster costs. Traffic between nodes in different VPC is always encrypted. Traffic between nodes in the same VPC is plaintext. Example with nodes on AWS, AZURE and GCP clouds:","title":"Cloud provided VPN"},{"location":"concepts/vpn-overview/#wireguard-vpn","text":"WireGuard VPN - CAST AI integrated alternative to Cloud provided VPN. This option is optimized for saving cost. You can choose between two topologies: Topology Description Full Mesh Traffic is encrypted between each node even if it is located in the same VPC. Cross Location Mesh Traffic is encrypted only between nodes in different VPC. Traffic in CAST AI platform: Each node runs a WireGuard kernel module. VPN peers configuration, keys exchange, network interfaces and routing tables are fully managed by the CAST AI platform. Each node has public IP for node-to-node communication. Firewall rules allow sending/receive UDP packets on 51820 port for each node. Each node is assigned a private IP from cloud's VPC subnet range and WireGuard interface IP from 10.4.0.0/16 subnet. Traffic between nodes in different VPC is always encrypted. Traffic between nodes in the same VPC is encrypted or plaintext depending on topology selection. Example with nodes on AWS, AZURE, GCP, and DIGITAL OCEAN clouds:","title":"WireGuard VPN"},{"location":"console-overview/api/","text":"API \u00b6 API documentation API authentication You can read more about our API here - API","title":"API"},{"location":"console-overview/api/#api","text":"API documentation API authentication You can read more about our API here - API","title":"API"},{"location":"console-overview/audit-log/","text":"Audit Log \u00b6 Audit log of cluster management on a high level. Select a date range for the log. View operations made and who initiated them.","title":"Audit Log"},{"location":"console-overview/audit-log/#audit-log","text":"Audit log of cluster management on a high level. Select a date range for the log. View operations made and who initiated them.","title":"Audit Log"},{"location":"console-overview/clusters/","text":"Clusters \u00b6 When you open any cluster from the /dashboard menu you will arrive at /clusters management. Here you will see more information about the selected cluster and will get access to the cluster management menu. Quickly navigate through active clusters. Information and log of the selected cluster. Management menu.","title":"Clusters"},{"location":"console-overview/clusters/#clusters","text":"When you open any cluster from the /dashboard menu you will arrive at /clusters management. Here you will see more information about the selected cluster and will get access to the cluster management menu. Quickly navigate through active clusters. Information and log of the selected cluster. Management menu.","title":"Clusters"},{"location":"console-overview/dashboard/","text":"Dashboard \u00b6 In the dashboard window, you will see all active and deleted clusters. Create a new cluster. Download kubeconfig of a cluster, pause, or delete it. You can open any specific cluster to manage its policies, add or remove nodes, check logs (see -> /clusters ). Copy cluster ID for API management. To unlock all the features in console you will need to create a cluster. See - Getting started section.","title":"Dashboard"},{"location":"console-overview/dashboard/#dashboard","text":"In the dashboard window, you will see all active and deleted clusters. Create a new cluster. Download kubeconfig of a cluster, pause, or delete it. You can open any specific cluster to manage its policies, add or remove nodes, check logs (see -> /clusters ). Copy cluster ID for API management. To unlock all the features in console you will need to create a cluster. See - Getting started section.","title":"Dashboard"},{"location":"console-overview/logs/","text":"Logs \u00b6 Kubernetes UI \u00b6 View more detailed information about the selected cluster and manage it in the Kubernetes UI . Kibana logs \u00b6 View Kibana logs of the selected cluster. Grafana logs \u00b6 View Grafana logs of the selected cluster.","title":"Logs"},{"location":"console-overview/logs/#logs","text":"","title":"Logs"},{"location":"console-overview/logs/#kubernetes-ui","text":"View more detailed information about the selected cluster and manage it in the Kubernetes UI .","title":"Kubernetes UI"},{"location":"console-overview/logs/#kibana-logs","text":"View Kibana logs of the selected cluster.","title":"Kibana logs"},{"location":"console-overview/logs/#grafana-logs","text":"View Grafana logs of the selected cluster.","title":"Grafana logs"},{"location":"console-overview/nodes/","text":"Nodes \u00b6 View information about the selected cluster nodes and manage them here. Add a new node. If your cluster runs on multiple clouds you will be able to specify a cloud provider for the node(s). Specify a CAST shape for the node(s) - a virtual specification of a Virtual Machine computing unit. Add multiple nodes at once (1-20). View information about nodes, copy node ID for API management, and delete nodes.","title":"Nodes"},{"location":"console-overview/nodes/#nodes","text":"View information about the selected cluster nodes and manage them here. Add a new node. If your cluster runs on multiple clouds you will be able to specify a cloud provider for the node(s). Specify a CAST shape for the node(s) - a virtual specification of a Virtual Machine computing unit. Add multiple nodes at once (1-20). View information about nodes, copy node ID for API management, and delete nodes.","title":"Nodes"},{"location":"console-overview/policies/","text":"Policies \u00b6 Manage policies for the selected cluster. Policies will help you optimize and reduce cost of your cloud bill and will automate the process of scaling up and down for you. 1. Cluster limits Cluster limits - policies that limit the cluster scale to the defined limits. This policy has the highest priority, and all the other policies cannot scale the cluster over the defined limits. CPU policy - This policy ensures that your cluster stays within the defined CPU minimum and maximum counts. Use this policy to create a guardrail against unexpected costs, in cases where traffic or workload requirements grow beyond budget expectations. 2. Pod autoscaler Pod autoscaler - policies that scale the cluster pods based on demand (e.g. CPU or memory utilization). This policy is second in priority, after the limits policy. Horizontal pod autoscaler (HPA) policy - This policy enables the Kubernetes Event Driven Autoscaler (KEDA) to automatically increase/decrease pod replica counts based on metrics. This enables cost savings by eliminating wasteful pods, and also ensures that your services are able to scale up to handle increased traffic and workload requirements. 3. Node autoscaler Node autoscaler - policies to scale cluster based on the CPU or memory demand Spot/Preemtive Instances policy - This policy enables the CAST optimization engine to purchase Spot (AWS / Azure) or Preemptive (GCP) instances when pods are labeled by the user. CAST automatically handles instance interruptions and replaces instances when they are terminated by the CSP. Spot instances typically yield savings of 60-80% and are useful for stateless workloads such as microservices. CAST AI currently supports AWS Spot instances, with GCP and Azure rolling out shortly. Unscheduled pods policy - This policy automatically adds nodes to your cluster so that your pods have a place to run. Both CPU and Memory requirements are considered. You can use CAST specified labels to ensure that your pods run in a specific Cloud, or let the CAST AI optimization engine choose for you. Node Deletion Policy - This policy will automatically remove nodes from your cluster when they no longer have scheduled workloads. This allows your cluster to maintain a minimal footprint and reduce cloud costs. For more information see: Autoscaling policies Horizontal Pod autoscaler Spot/Preemptible Instances","title":"Policies"},{"location":"console-overview/policies/#policies","text":"Manage policies for the selected cluster. Policies will help you optimize and reduce cost of your cloud bill and will automate the process of scaling up and down for you. 1. Cluster limits Cluster limits - policies that limit the cluster scale to the defined limits. This policy has the highest priority, and all the other policies cannot scale the cluster over the defined limits. CPU policy - This policy ensures that your cluster stays within the defined CPU minimum and maximum counts. Use this policy to create a guardrail against unexpected costs, in cases where traffic or workload requirements grow beyond budget expectations. 2. Pod autoscaler Pod autoscaler - policies that scale the cluster pods based on demand (e.g. CPU or memory utilization). This policy is second in priority, after the limits policy. Horizontal pod autoscaler (HPA) policy - This policy enables the Kubernetes Event Driven Autoscaler (KEDA) to automatically increase/decrease pod replica counts based on metrics. This enables cost savings by eliminating wasteful pods, and also ensures that your services are able to scale up to handle increased traffic and workload requirements. 3. Node autoscaler Node autoscaler - policies to scale cluster based on the CPU or memory demand Spot/Preemtive Instances policy - This policy enables the CAST optimization engine to purchase Spot (AWS / Azure) or Preemptive (GCP) instances when pods are labeled by the user. CAST automatically handles instance interruptions and replaces instances when they are terminated by the CSP. Spot instances typically yield savings of 60-80% and are useful for stateless workloads such as microservices. CAST AI currently supports AWS Spot instances, with GCP and Azure rolling out shortly. Unscheduled pods policy - This policy automatically adds nodes to your cluster so that your pods have a place to run. Both CPU and Memory requirements are considered. You can use CAST specified labels to ensure that your pods run in a specific Cloud, or let the CAST AI optimization engine choose for you. Node Deletion Policy - This policy will automatically remove nodes from your cluster when they no longer have scheduled workloads. This allows your cluster to maintain a minimal footprint and reduce cloud costs. For more information see: Autoscaling policies Horizontal Pod autoscaler Spot/Preemptible Instances","title":"Policies"},{"location":"console-overview/schedule/","text":"Cluster schedule \u00b6 Control when your cluster should be running to match your specific needs. Clusters can be paused and resumed during your predefined time periods in order to save resources and money. By default there is no schedule set on your cluster. Key functionality: Time zone selector. Select a time zone (default time zone is set the same as on your machine) Select the weekdays when you wish to run your cluster based on a schedule, during other weekdays cluster will be in a paused state. Set active time for your cluster based on: Weekday Time period in the day. Multiple time intervals can be set in a single day to cater for custom work patters, breaks etc. Cluster can be set to be always \u201cON\u201d (i.e. 24hrs). Copy schedule set in one day to one or more other weekdays. Check estimated savings to understand how much you can save if schedule (as defined) will be applied on a cluster. Save changes. Cluster will be paused and resumed based on values entered. Delete schedule (only available if schedule was set to begin with). Cluster will have no schedule applied and will be active 24/7.","title":"Cluster schedule"},{"location":"console-overview/schedule/#cluster-schedule","text":"Control when your cluster should be running to match your specific needs. Clusters can be paused and resumed during your predefined time periods in order to save resources and money. By default there is no schedule set on your cluster. Key functionality: Time zone selector. Select a time zone (default time zone is set the same as on your machine) Select the weekdays when you wish to run your cluster based on a schedule, during other weekdays cluster will be in a paused state. Set active time for your cluster based on: Weekday Time period in the day. Multiple time intervals can be set in a single day to cater for custom work patters, breaks etc. Cluster can be set to be always \u201cON\u201d (i.e. 24hrs). Copy schedule set in one day to one or more other weekdays. Check estimated savings to understand how much you can save if schedule (as defined) will be applied on a cluster. Save changes. Cluster will be paused and resumed based on values entered. Delete schedule (only available if schedule was set to begin with). Cluster will have no schedule applied and will be active 24/7.","title":"Cluster schedule"},{"location":"getting-started/create-cluster/","text":"2. Create cluster \u00b6 Once you add cloud credentials - you are ready to create a cluster. In the cluster creation window you will have a few options to lay a base foundation for your cluster, which we will be able to further customize to your needs once a cluster is up and running. Cluster details \u00b6 The new cluster will be created with the name and in the region, you specify here. After the cluster is created, name and region cannot be changed. Name your cluster (2-50 symbols, only letters, numbers, and hyphens allowed) Select a region where your cluster will be located Cluster configuration \u00b6 Select initial cluster configuration. It may be automatically adjusted based on scaling and cost optimization policies. You will be able to adjust policies once the cluster is created. You may also manually add nodes once the cluster is provisioned. Cloud providers \u00b6 Select the cloud provider(s) you would like to use for this cluster. You will need to select credentials that you would like to use for each provider, please refer to 1. Add cloud credentials section if you have no credentials added. Cluster virtual private network \u00b6 Select preferred encrypted connection type. Cloud provided VPN is a default VPN provided by the respective cloud service providers. WireGuard is a CAST AI integrated choice of VPN that cuts cloud cost. WireGuard VPN: Full Mesh - network traffic is encrypted between all nodes WireGuard VPN: Cross Location Mesh - network traffic is encrypted only between nodes in different clouds Cloud provided VPN - default network encryption provided by selected CSPs Also see - VPN overview Next step: deploy application","title":"2. Create cluster"},{"location":"getting-started/create-cluster/#2-create-cluster","text":"Once you add cloud credentials - you are ready to create a cluster. In the cluster creation window you will have a few options to lay a base foundation for your cluster, which we will be able to further customize to your needs once a cluster is up and running.","title":"2. Create cluster"},{"location":"getting-started/create-cluster/#cluster-details","text":"The new cluster will be created with the name and in the region, you specify here. After the cluster is created, name and region cannot be changed. Name your cluster (2-50 symbols, only letters, numbers, and hyphens allowed) Select a region where your cluster will be located","title":"Cluster details"},{"location":"getting-started/create-cluster/#cluster-configuration","text":"Select initial cluster configuration. It may be automatically adjusted based on scaling and cost optimization policies. You will be able to adjust policies once the cluster is created. You may also manually add nodes once the cluster is provisioned.","title":"Cluster configuration"},{"location":"getting-started/create-cluster/#cloud-providers","text":"Select the cloud provider(s) you would like to use for this cluster. You will need to select credentials that you would like to use for each provider, please refer to 1. Add cloud credentials section if you have no credentials added.","title":"Cloud providers"},{"location":"getting-started/create-cluster/#cluster-virtual-private-network","text":"Select preferred encrypted connection type. Cloud provided VPN is a default VPN provided by the respective cloud service providers. WireGuard is a CAST AI integrated choice of VPN that cuts cloud cost. WireGuard VPN: Full Mesh - network traffic is encrypted between all nodes WireGuard VPN: Cross Location Mesh - network traffic is encrypted only between nodes in different clouds Cloud provided VPN - default network encryption provided by selected CSPs Also see - VPN overview Next step: deploy application","title":"Cluster virtual private network"},{"location":"getting-started/deploy-application/","text":"3. Deploy application \u00b6 CAST AI managed cluster runs on Kubernetes. Once you create a cluster - you can download a kubeconfig file of the cluster and deploy your application using kubectl command-line tool. For more information please refer to Kubernetes documentation . Relevant for this section: Kubernetes docs - Organizing cluster access using kubeconfig files Kubernetes docs - Deploy an app using kubectl","title":"3. Deploy application"},{"location":"getting-started/deploy-application/#3-deploy-application","text":"CAST AI managed cluster runs on Kubernetes. Once you create a cluster - you can download a kubeconfig file of the cluster and deploy your application using kubectl command-line tool. For more information please refer to Kubernetes documentation . Relevant for this section: Kubernetes docs - Organizing cluster access using kubeconfig files Kubernetes docs - Deploy an app using kubectl","title":"3. Deploy application"},{"location":"getting-started/overview/","text":"Overview \u00b6 This guide will help you get started and deploy your first cluster with CAST AI. To start using CAST AI you will need: An account - sign up here Cloud credentials - join slack and claim free trial An application developed on Kubernetes Estimated time to get started - 10 minutes. Add cloud credentials Create cluster Deploy application","title":"Overview"},{"location":"getting-started/overview/#overview","text":"This guide will help you get started and deploy your first cluster with CAST AI. To start using CAST AI you will need: An account - sign up here Cloud credentials - join slack and claim free trial An application developed on Kubernetes Estimated time to get started - 10 minutes. Add cloud credentials Create cluster Deploy application","title":"Overview"},{"location":"getting-started/credentials/configuring-aws-credentials/","text":"AWS \u00b6 To add AWS credentials you will need: Access key ID , Secret access key . Add a new user Open https://console.aws.amazon.com Open the IAM service, then go to Users and click on Add user Select Programmatic access Create a new group Select the following permissions: AmazonVPCFullAccess, AmazonEC2FullAccess, IAMFullAccess Paste cloud credentials Once you reach the last page (\"Create user\"), copy the access key ID and secret access key Navigate to cloud credentials page in CAST AI console and select AWS Paste keys to the form in CAST AI console and click create Documentation on AWS Identity and Access Management. Next step: create cluster","title":"AWS"},{"location":"getting-started/credentials/configuring-aws-credentials/#aws","text":"To add AWS credentials you will need: Access key ID , Secret access key . Add a new user Open https://console.aws.amazon.com Open the IAM service, then go to Users and click on Add user Select Programmatic access Create a new group Select the following permissions: AmazonVPCFullAccess, AmazonEC2FullAccess, IAMFullAccess Paste cloud credentials Once you reach the last page (\"Create user\"), copy the access key ID and secret access key Navigate to cloud credentials page in CAST AI console and select AWS Paste keys to the form in CAST AI console and click create Documentation on AWS Identity and Access Management. Next step: create cluster","title":"AWS"},{"location":"getting-started/credentials/configuring-azure-credentials/","text":"Azure \u00b6 Method 1: Create using az cli utility \u00b6 Prerequisites (recommended) Visit Azure Portal and open Cloud Shell at the top right side of menu bar. (alternative) You can also use your local az cli installation. Generate service principal \u00b6 Run the script displayed below. It will create a new service principal with required roles, enable access to CAST Image Gallery and print your credentials json. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/castai/docs/main/docs/getting-started/credentials/configuring-azure-credentials/script.sh ) \" You'll see the following output: user@Azure:~$ /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/castai/docs/main/docs/getting-started/credentials/configuring-azure-credentials/script.sh ) \" == Setup Available subscriptions: ID NAME IS_DEFAULT -- ---- ---------- XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX Free Trial true Enter subscription id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX Press enter to continue .. == Importing CAST.AI image gallery . Using existing service principal == Registering app . Creating app registration . Using existing service principal == Assigning roles to the app . Assigning Contributor role to CAST.AI app . Assigning Contributor role to CAST.AI shared images app -------------------------------------------------------------------------------- Save and use the following json to onboard credentials into CAST.AI { \"subscriptionId\" : \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\" , \"tenantId\" : \"YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY\" , \"clientId\" : \"ZZZZZZZZ-ZZZZ-ZZZZ-ZZZZ-ZZZZZZZZZZZZ\" , \"clientSecret\" : \"FX~JXqv~~~uiewDHJKDH9333d~ZZdf\" } Copy the displayed JSON Open cloud credentials in CAST AI console Select Azure Paste the values into relevant fields Next step: create cluster Method 2: Create it manually using the Azure portal \u00b6 To get started, you need to create an Active Directory (AD) service principal in your Azure account and assign the required permissions. Create App registration Open https://portal.azure.com Go to App registrations, and click on New registration. Enter display name and click Register. Go to cloud credentials in CAST AI console and select Azure Paste in the Directory (tenant) ID to the form on the left side. Paste in the Application (client) ID to the form on the left side. Select Certificates & secrets in the left sidebar menu. Create a new client secret without expiration. Paste in the new client secret value to the form on the left side, to the Client Secret field. Give access to the CAST AI application by requesting a sign-in using a browser Accept CAST AI application. After Sign-in you should see Permissions requested window. Click Accept which will allow you to add the CAST AI application role. Assign the roles Open Subscriptions page and go to your subscription. Paste in the Subscription ID to the form on the left side. Select the Access Control (IAM) in the left sidebar menu. Add the role assignment with Role: Contributor, and in the Select search field type your Client Secret (created during the first step). Add another role assignment with Role: Contributor, and in the Select input field search for CAST AI Shared Images then click save (if the role is not visible please check previous step and try again). Documentation on Azure Cloud EKS IAM Policies, Roles, and Permissions. Next step: create cluster","title":"Azure"},{"location":"getting-started/credentials/configuring-azure-credentials/#azure","text":"","title":"Azure"},{"location":"getting-started/credentials/configuring-azure-credentials/#method-1-create-using-az-cli-utility","text":"Prerequisites (recommended) Visit Azure Portal and open Cloud Shell at the top right side of menu bar. (alternative) You can also use your local az cli installation.","title":"Method 1: Create using az cli utility"},{"location":"getting-started/credentials/configuring-azure-credentials/#generate-service-principal","text":"Run the script displayed below. It will create a new service principal with required roles, enable access to CAST Image Gallery and print your credentials json. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/castai/docs/main/docs/getting-started/credentials/configuring-azure-credentials/script.sh ) \" You'll see the following output: user@Azure:~$ /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/castai/docs/main/docs/getting-started/credentials/configuring-azure-credentials/script.sh ) \" == Setup Available subscriptions: ID NAME IS_DEFAULT -- ---- ---------- XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX Free Trial true Enter subscription id: XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX Press enter to continue .. == Importing CAST.AI image gallery . Using existing service principal == Registering app . Creating app registration . Using existing service principal == Assigning roles to the app . Assigning Contributor role to CAST.AI app . Assigning Contributor role to CAST.AI shared images app -------------------------------------------------------------------------------- Save and use the following json to onboard credentials into CAST.AI { \"subscriptionId\" : \"XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\" , \"tenantId\" : \"YYYYYYYY-YYYY-YYYY-YYYY-YYYYYYYYYYYY\" , \"clientId\" : \"ZZZZZZZZ-ZZZZ-ZZZZ-ZZZZ-ZZZZZZZZZZZZ\" , \"clientSecret\" : \"FX~JXqv~~~uiewDHJKDH9333d~ZZdf\" } Copy the displayed JSON Open cloud credentials in CAST AI console Select Azure Paste the values into relevant fields Next step: create cluster","title":"Generate service principal"},{"location":"getting-started/credentials/configuring-azure-credentials/#method-2-create-it-manually-using-the-azure-portal","text":"To get started, you need to create an Active Directory (AD) service principal in your Azure account and assign the required permissions. Create App registration Open https://portal.azure.com Go to App registrations, and click on New registration. Enter display name and click Register. Go to cloud credentials in CAST AI console and select Azure Paste in the Directory (tenant) ID to the form on the left side. Paste in the Application (client) ID to the form on the left side. Select Certificates & secrets in the left sidebar menu. Create a new client secret without expiration. Paste in the new client secret value to the form on the left side, to the Client Secret field. Give access to the CAST AI application by requesting a sign-in using a browser Accept CAST AI application. After Sign-in you should see Permissions requested window. Click Accept which will allow you to add the CAST AI application role. Assign the roles Open Subscriptions page and go to your subscription. Paste in the Subscription ID to the form on the left side. Select the Access Control (IAM) in the left sidebar menu. Add the role assignment with Role: Contributor, and in the Select search field type your Client Secret (created during the first step). Add another role assignment with Role: Contributor, and in the Select input field search for CAST AI Shared Images then click save (if the role is not visible please check previous step and try again). Documentation on Azure Cloud EKS IAM Policies, Roles, and Permissions. Next step: create cluster","title":"Method 2: Create it manually using the Azure portal"},{"location":"getting-started/credentials/configuring-digitalocean-credentials/","text":"Digital Ocean \u00b6 To get started, you need to create a Personal Access Token and define its access permissions. Sign into your Digital Ocean account Click the API tab on the left sidebar at the bottom API tokens Click Generate New Token in the Personal Access Token section Add a name and select both the read and write scopes Click Generate Token The token will be displayed only once under the name you gave it Go to cloud credentials in CAST AI console, select Digital Ocean and paste Token to the form Next step: create cluster","title":"Digital Ocean"},{"location":"getting-started/credentials/configuring-digitalocean-credentials/#digital-ocean","text":"To get started, you need to create a Personal Access Token and define its access permissions. Sign into your Digital Ocean account Click the API tab on the left sidebar at the bottom API tokens Click Generate New Token in the Personal Access Token section Add a name and select both the read and write scopes Click Generate Token The token will be displayed only once under the name you gave it Go to cloud credentials in CAST AI console, select Digital Ocean and paste Token to the form Next step: create cluster","title":"Digital Ocean"},{"location":"getting-started/credentials/configuring-gcp-credentials/","text":"GCP \u00b6 By following these instructions, you\u2019ll retrieve the service account JSON credentials. These credentials are required by CAST AI for creating a cluster with GCP resources. Method 1: Create using gcloud utility \u00b6 Prerequisites \u00b6 (recommended) Visit the Google Cloud Platform https://console.cloud.google.com/ and make sure you have selected the right project. Project needs to be precreated with billing account linked (can be free $300 trial). Open Cloud Shell at the top right side of menu bar. (alternative) You can also use your local gcloud installation. Make sure that you have activated the right project ( gcloud projects list and gcloud config set project {{desired-project-id-here}} ) Generate service account \u00b6 Run the script displayed below. It will create a new service account with the required permissions, enable the required APIs, and print your service account key JSON. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/castai/docs/main/docs/getting-started/credentials/configuring-gcp-credentials/script.sh ) \" You'll see the following output: user@cloudshell:~ ( project ) $ /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/castai/docs/main/docs/getting-started/credentials/configuring-gcp-credentials/script.sh ) \" Your active configuration is: [ cloudshell-21130 ] Setting up GCP cloud credentials PROJECT_ID = project SERVICE_ACCOUNT_ID = castai-credentials-1613140179 SERVICE_ACCOUNT_EMAIL = castai-credentials-1613140179@project.iam.gserviceaccount.com Enabling required google cloud apis Creating service account Generating service account key Assigning required roles to castai-credentials-1613140179@project.iam.gserviceaccount.com service account - Assigning roles/compute.admin - Assigning roles/iam.serviceAccountUser - Assigning roles/iam.serviceAccountAdmin - Assigning roles/iam.roleAdmin - Assigning roles/iam.serviceAccountKeyAdmin - Assigning roles/resourcemanager.projectIamAdmin Activating castai-credentials-1613140179@project.iam.gserviceaccount.com service account Service account key json: { \"type\" : \"service_account\" , \"project_id\" : \"project\" , \"private_key_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"private_key\" : \"-----BEGIN PRIVATE KEY-----\\nxxxxxxxxxxxxxxxxxxxxxx\\n-----END PRIVATE KEY-----\\n\" , \"client_email\" : \"castai-credentials-1613140179@project.iam.gserviceaccount.com\" , \"client_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\" , \"token_uri\" : \"https://oauth2.googleapis.com/token\" , \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\" , \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/castai-credentials-1613140179%40project.iam.gserviceaccount.com\" } Copy the displayed JSON Open cloud credentials in CAST AI console Select Google Cloud Platform Paste it in the Service Account JSON field. Cloud Shell will copy a selected text automatically. Don't use ctrl+c as \"private_key\" part of the JSON might get corrupted due to word wrapping. Next step: create cluster Method 2: Create it manually using the Google Cloud Console \u00b6 Project prerequisites \u00b6 Note that the project where you created your service account needs to have the following APIs enabled: IAM API Compute API Resource Manager API Please follow the GCP guide to learn more about how to enable APIs. Create service account \u00b6 Open https://console.cloud.google.com/ Select your project (or create a new one) in the top bar. Go to the Navigation bar, select IAM & Admin , and then Service accounts : Click Create service account : Enter the preferred Service account name and description . Click Create Add the following roles to the created account: roles/compute.admin roles/iam.serviceAccountUser roles/iam.serviceAccountAdmin roles/iam.roleAdministrator roles/iam.serviceAccountKey roles/iam.projectIAMAdmin Click Save . In the last step of the service account creation, click Done without entering any data. Create key \u00b6 The created account will appear in the service accounts list. Click on it to access additional options. In the Keys section, click on Add Key \u2192 Create new key . Select the JSON option and click Create . You will get a file download prompt. After the JSON file is downloaded open cloud credentials in CAST AI console and select Google Cloud platform Copy file contents to the input field or click on the Read from file button to import the file. Next step: create cluster","title":"GCP"},{"location":"getting-started/credentials/configuring-gcp-credentials/#gcp","text":"By following these instructions, you\u2019ll retrieve the service account JSON credentials. These credentials are required by CAST AI for creating a cluster with GCP resources.","title":"GCP"},{"location":"getting-started/credentials/configuring-gcp-credentials/#method-1-create-using-gcloud-utility","text":"","title":"Method 1: Create using gcloud utility"},{"location":"getting-started/credentials/configuring-gcp-credentials/#prerequisites","text":"(recommended) Visit the Google Cloud Platform https://console.cloud.google.com/ and make sure you have selected the right project. Project needs to be precreated with billing account linked (can be free $300 trial). Open Cloud Shell at the top right side of menu bar. (alternative) You can also use your local gcloud installation. Make sure that you have activated the right project ( gcloud projects list and gcloud config set project {{desired-project-id-here}} )","title":"Prerequisites"},{"location":"getting-started/credentials/configuring-gcp-credentials/#generate-service-account","text":"Run the script displayed below. It will create a new service account with the required permissions, enable the required APIs, and print your service account key JSON. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/castai/docs/main/docs/getting-started/credentials/configuring-gcp-credentials/script.sh ) \" You'll see the following output: user@cloudshell:~ ( project ) $ /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/castai/docs/main/docs/getting-started/credentials/configuring-gcp-credentials/script.sh ) \" Your active configuration is: [ cloudshell-21130 ] Setting up GCP cloud credentials PROJECT_ID = project SERVICE_ACCOUNT_ID = castai-credentials-1613140179 SERVICE_ACCOUNT_EMAIL = castai-credentials-1613140179@project.iam.gserviceaccount.com Enabling required google cloud apis Creating service account Generating service account key Assigning required roles to castai-credentials-1613140179@project.iam.gserviceaccount.com service account - Assigning roles/compute.admin - Assigning roles/iam.serviceAccountUser - Assigning roles/iam.serviceAccountAdmin - Assigning roles/iam.roleAdmin - Assigning roles/iam.serviceAccountKeyAdmin - Assigning roles/resourcemanager.projectIamAdmin Activating castai-credentials-1613140179@project.iam.gserviceaccount.com service account Service account key json: { \"type\" : \"service_account\" , \"project_id\" : \"project\" , \"private_key_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"private_key\" : \"-----BEGIN PRIVATE KEY-----\\nxxxxxxxxxxxxxxxxxxxxxx\\n-----END PRIVATE KEY-----\\n\" , \"client_email\" : \"castai-credentials-1613140179@project.iam.gserviceaccount.com\" , \"client_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\" , \"token_uri\" : \"https://oauth2.googleapis.com/token\" , \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\" , \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/castai-credentials-1613140179%40project.iam.gserviceaccount.com\" } Copy the displayed JSON Open cloud credentials in CAST AI console Select Google Cloud Platform Paste it in the Service Account JSON field. Cloud Shell will copy a selected text automatically. Don't use ctrl+c as \"private_key\" part of the JSON might get corrupted due to word wrapping. Next step: create cluster","title":"Generate service account"},{"location":"getting-started/credentials/configuring-gcp-credentials/#method-2-create-it-manually-using-the-google-cloud-console","text":"","title":"Method 2: Create it manually using the Google Cloud Console"},{"location":"getting-started/credentials/configuring-gcp-credentials/#project-prerequisites","text":"Note that the project where you created your service account needs to have the following APIs enabled: IAM API Compute API Resource Manager API Please follow the GCP guide to learn more about how to enable APIs.","title":"Project prerequisites"},{"location":"getting-started/credentials/configuring-gcp-credentials/#create-service-account","text":"Open https://console.cloud.google.com/ Select your project (or create a new one) in the top bar. Go to the Navigation bar, select IAM & Admin , and then Service accounts : Click Create service account : Enter the preferred Service account name and description . Click Create Add the following roles to the created account: roles/compute.admin roles/iam.serviceAccountUser roles/iam.serviceAccountAdmin roles/iam.roleAdministrator roles/iam.serviceAccountKey roles/iam.projectIAMAdmin Click Save . In the last step of the service account creation, click Done without entering any data.","title":"Create service account"},{"location":"getting-started/credentials/configuring-gcp-credentials/#create-key","text":"The created account will appear in the service accounts list. Click on it to access additional options. In the Keys section, click on Add Key \u2192 Create new key . Select the JSON option and click Create . You will get a file download prompt. After the JSON file is downloaded open cloud credentials in CAST AI console and select Google Cloud platform Copy file contents to the input field or click on the Read from file button to import the file. Next step: create cluster","title":"Create key"},{"location":"getting-started/credentials/credentials-overview/","text":"Information \u00b6 You can skip this step if you have a free trial activated - jump to create cluster CAST AI will need your cloud credentials to call cloud APIs, so the platform can create, orchestrate and optimize clusters for you. CAST AI does not store your credentials or password. You can opt-out and remove them any time you want. If you remove the credentials - you are free to use the Kubernetes provisioned cluster as it is but you will lose all the managed service benefits and features that CAST AI offers. Guides to add credentials: Amazon Web Services Azure Digital Ocean Google Cloud Platform","title":"Information"},{"location":"getting-started/credentials/credentials-overview/#information","text":"You can skip this step if you have a free trial activated - jump to create cluster CAST AI will need your cloud credentials to call cloud APIs, so the platform can create, orchestrate and optimize clusters for you. CAST AI does not store your credentials or password. You can opt-out and remove them any time you want. If you remove the credentials - you are free to use the Kubernetes provisioned cluster as it is but you will lose all the managed service benefits and features that CAST AI offers. Guides to add credentials: Amazon Web Services Azure Digital Ocean Google Cloud Platform","title":"Information"},{"location":"getting-started/external-cluster/eks/","text":"AWS EKS \u00b6 Connect cluster \u00b6 To connect your EKS cluster, login to the CAST console and navigate to the Connect cluster window. Copy the following script and run it your terminal or cloud shell. Make sure that kubectl is installed and can access your cluster. After installation, your EKS cluster should appear in the cluster list. From there, you can open cluster details and explore a detailed savings estimate based on your cluster configuration. Agent will run in read-only mode providing saving suggestions without any actual modifications. Credential onboarding \u00b6 To unlock all the benefits and enable automatic cost optimization, CAST AI must have access to your cluster. The following section will describe the steps required to onboard the EKS cluster on the CAST AI console. To make it less troublesome, we have created script that automates most of the steps. Prerequisites: AWS CLI - A command line tool for working with AWS services using commands in your command-line shell. For more information, see Installing AWS CLI . eksctl \u2013 A command line tool for working with EKS clusters that automates many individual tasks. This guide requires that you use version 0.43.0 or later. For more information, see The eksctl command line utility . IAM permissions \u2013 The IAM security principal that you're using must have permissions to work with AWS EKS, AWS IAM, and related resources. Additionally, you should have access to the EKS cluster that you wish to onboard on the CAST AI console. CAST AI agent has to be running on the cluster. Onboarding steps: To onboard your cluster go to \u201cAvailable Savings\u201d report and click on \u201cStart saving\u201d or \u201cEnable CAST AI\u201d button. The actual button will depend on an amount of optimizations available from your cluster. Follow the instruction in the pop-up window to create & use AWS AccessKeyId and SecretAccessKey The script will create a new AWS user with the required permissions, modify aws-auth ConfigMap, and print AWS AccessKeyId and SecretAccessKey , which then can be added to the CAST AI console and assigned to the corresponding EKS cluster. Generated user will have following permissions: AmazonEC2ReadOnlyAccess IAMReadOnlyAccess Manage instances in specified cluster restricted to cluster VPC Manage autoscaling groups in specified cluster Manage EKS Node Groups in specified cluster All Write permissions are scoped to single EKS cluster, and it won't have access to resources of any other clusters in the AWS account. That\u2019s it! Your cluster is onboarded. You can now enable policies to keep your cluster configuration optimal. To complete steps mentioned above manually (without our script) be aware that when you create an Amazon EKS cluster, the IAM entity user or role, such as a federated user that creates the cluster, is automatically granted a system:masters permissions in the cluster's RBAC configuration in the control plane. To grant additional AWS users or roles the ability to interact with your cluster, you must edit the aws-auth ConfigMap within Kubernetes. For more information, see Managing users or IAM roles for your cluster .","title":"AWS EKS"},{"location":"getting-started/external-cluster/eks/#aws-eks","text":"","title":"AWS EKS"},{"location":"getting-started/external-cluster/eks/#connect-cluster","text":"To connect your EKS cluster, login to the CAST console and navigate to the Connect cluster window. Copy the following script and run it your terminal or cloud shell. Make sure that kubectl is installed and can access your cluster. After installation, your EKS cluster should appear in the cluster list. From there, you can open cluster details and explore a detailed savings estimate based on your cluster configuration. Agent will run in read-only mode providing saving suggestions without any actual modifications.","title":"Connect cluster"},{"location":"getting-started/external-cluster/eks/#credential-onboarding","text":"To unlock all the benefits and enable automatic cost optimization, CAST AI must have access to your cluster. The following section will describe the steps required to onboard the EKS cluster on the CAST AI console. To make it less troublesome, we have created script that automates most of the steps. Prerequisites: AWS CLI - A command line tool for working with AWS services using commands in your command-line shell. For more information, see Installing AWS CLI . eksctl \u2013 A command line tool for working with EKS clusters that automates many individual tasks. This guide requires that you use version 0.43.0 or later. For more information, see The eksctl command line utility . IAM permissions \u2013 The IAM security principal that you're using must have permissions to work with AWS EKS, AWS IAM, and related resources. Additionally, you should have access to the EKS cluster that you wish to onboard on the CAST AI console. CAST AI agent has to be running on the cluster. Onboarding steps: To onboard your cluster go to \u201cAvailable Savings\u201d report and click on \u201cStart saving\u201d or \u201cEnable CAST AI\u201d button. The actual button will depend on an amount of optimizations available from your cluster. Follow the instruction in the pop-up window to create & use AWS AccessKeyId and SecretAccessKey The script will create a new AWS user with the required permissions, modify aws-auth ConfigMap, and print AWS AccessKeyId and SecretAccessKey , which then can be added to the CAST AI console and assigned to the corresponding EKS cluster. Generated user will have following permissions: AmazonEC2ReadOnlyAccess IAMReadOnlyAccess Manage instances in specified cluster restricted to cluster VPC Manage autoscaling groups in specified cluster Manage EKS Node Groups in specified cluster All Write permissions are scoped to single EKS cluster, and it won't have access to resources of any other clusters in the AWS account. That\u2019s it! Your cluster is onboarded. You can now enable policies to keep your cluster configuration optimal. To complete steps mentioned above manually (without our script) be aware that when you create an Amazon EKS cluster, the IAM entity user or role, such as a federated user that creates the cluster, is automatically granted a system:masters permissions in the cluster's RBAC configuration in the control plane. To grant additional AWS users or roles the ability to interact with your cluster, you must edit the aws-auth ConfigMap within Kubernetes. For more information, see Managing users or IAM roles for your cluster .","title":"Credential onboarding"},{"location":"getting-started/external-cluster/overview/","text":"Overview \u00b6 External cluster management brings CAST AI features, like autoscaler, to an externally managed clusters, like EKS, GKE or AKS. By installing CAST AI agent, you start observing cluster running costs and potential savings; you can then enable features that optimize your cluster - like adding and removing nodes, or right-sizing deployments. To get started, login to the console and navigate to Connect cluster window. Script will install the agent that will run inside the cluster in read-only mode. After installation, agent will collect and analyze your cluster configuration to provide most optimal setup along with savings estimation for your current cloud environment. To start saving costs, turn on the automatic optimization when ready. Connect your cluster: AWS EKS GKE (Coming soon) AKS (Coming soon)","title":"Overview"},{"location":"getting-started/external-cluster/overview/#overview","text":"External cluster management brings CAST AI features, like autoscaler, to an externally managed clusters, like EKS, GKE or AKS. By installing CAST AI agent, you start observing cluster running costs and potential savings; you can then enable features that optimize your cluster - like adding and removing nodes, or right-sizing deployments. To get started, login to the console and navigate to Connect cluster window. Script will install the agent that will run inside the cluster in read-only mode. After installation, agent will collect and analyze your cluster configuration to provide most optimal setup along with savings estimation for your current cloud environment. To start saving costs, turn on the automatic optimization when ready. Connect your cluster: AWS EKS GKE (Coming soon) AKS (Coming soon)","title":"Overview"},{"location":"guides/autoscaling-policies/","text":"Autoscaling policies \u00b6 Autoscaling policies define a set of rules based on which your cluster is monitored and scaled to maintain steady performance at the lowest possible cost. This topic describes the available policy configuration options and provides guidance on how to configure them. Prerequisites \u00b6 CAST AI cluster - see create cluster . Select a cluster and navigate to the Policies menu. Cluster CPU limits policy \u00b6 Each CAST AI cluster size can be limited by the total amount of vCPUs available on all the worker nodes used to run workloads. If disabled, the cluster can upscale indefinitely and downscale to 0 worker nodes, depending on the actual resource consumption. Configuring CPU limits policy \u00b6 You can adjust a cluster's CPU limits settings either via the CAST AI console: or via the CAST AI policies API endpoint by setting values for \"clusterLimits\" : { \"cpu\" : { \"maxCores\" : <value> , \"minCores\" : <value> }, \"enabled\" : <value> } The new settings will propagate immediately. Horizontal Pod Autoscaler (HPA) policy \u00b6 See HPA documentation for a detailed overview. Node deletion policy \u00b6 This policy will automatically remove nodes from your cluster when they no longer have scheduled workloads. This allows your cluster to maintain a minimal footprint and reduce cloud costs. Unscheduled pods policy \u00b6 A pod becomes unschedulable when the Kubernetes scheduler cannot find a node to assign the pod to. For instance, a pod can request more CPU or memory than the resources available on any of the worker nodes. In many such cases, this indicates the need to scale up by adding additional nodes to the cluster. The CAST AI autoscaler is equipped with a mechanism to handle this. Headroom attributes \u00b6 Headroom is a buffer of spare capacity (in terms of both memory and CPU) to ensure that cluster can meet suddenly increased demand for resources. It is based on the currently available total worker nodes resource capacity. For example, if headroom for memory and CPU are both set to 10%, and the cluster consists of 2 worker nodes equipped with 2 cores and 4GB RAM each, a total of 0.4 cores and 819MB would be considered as headroom in the next cluster size increase phase. Provisioning decision \u00b6 After receiving the unschedulable pods event, the CAST AI recommendation engine will select the best price/performance ratio node capable of accommodating all of the currently unschedulable pods plus headroom. CAST AI will then provision it and join with the cluster. This process usually takes a few minutes, depending on the cloud service provider of your choice. Currently, only a single node will be added at a time. If any unschedulable pods still remain, the cycle is repeated until all the pods are scheduled (provided that the reason was insufficient resources). Configuring the unscheduled pod's policy \u00b6 You can enable/disable the unschedulable pod's policy and set headroom settings either on the CAST AI console : or via the CAST AI policies API endpoint by setting values for \"unschedulablePods\" : { \"enabled\" : <value> , \"headroom\" : { \"cpuPercentage\" : <value> , \"memoryPercentage\" : <value> } } It may take a few minutes for the new settings to propagate. Policies precedence rules \u00b6 If multiple policies are enabled and multiple rules are triggered during the same evaluation period, they will be handled in the following order: Cluster CPU limits policy Horizontal Pod Autoscaler (HPA) policy Unscheduled pods policy Node deletion policy","title":"Autoscaling policies"},{"location":"guides/autoscaling-policies/#autoscaling-policies","text":"Autoscaling policies define a set of rules based on which your cluster is monitored and scaled to maintain steady performance at the lowest possible cost. This topic describes the available policy configuration options and provides guidance on how to configure them.","title":"Autoscaling policies"},{"location":"guides/autoscaling-policies/#prerequisites","text":"CAST AI cluster - see create cluster . Select a cluster and navigate to the Policies menu.","title":"Prerequisites"},{"location":"guides/autoscaling-policies/#cluster-cpu-limits-policy","text":"Each CAST AI cluster size can be limited by the total amount of vCPUs available on all the worker nodes used to run workloads. If disabled, the cluster can upscale indefinitely and downscale to 0 worker nodes, depending on the actual resource consumption.","title":"Cluster CPU limits policy"},{"location":"guides/autoscaling-policies/#configuring-cpu-limits-policy","text":"You can adjust a cluster's CPU limits settings either via the CAST AI console: or via the CAST AI policies API endpoint by setting values for \"clusterLimits\" : { \"cpu\" : { \"maxCores\" : <value> , \"minCores\" : <value> }, \"enabled\" : <value> } The new settings will propagate immediately.","title":"Configuring CPU limits policy"},{"location":"guides/autoscaling-policies/#horizontal-pod-autoscaler-hpa-policy","text":"See HPA documentation for a detailed overview.","title":"Horizontal Pod Autoscaler (HPA) policy"},{"location":"guides/autoscaling-policies/#node-deletion-policy","text":"This policy will automatically remove nodes from your cluster when they no longer have scheduled workloads. This allows your cluster to maintain a minimal footprint and reduce cloud costs.","title":"Node deletion policy"},{"location":"guides/autoscaling-policies/#unscheduled-pods-policy","text":"A pod becomes unschedulable when the Kubernetes scheduler cannot find a node to assign the pod to. For instance, a pod can request more CPU or memory than the resources available on any of the worker nodes. In many such cases, this indicates the need to scale up by adding additional nodes to the cluster. The CAST AI autoscaler is equipped with a mechanism to handle this.","title":"Unscheduled pods policy"},{"location":"guides/autoscaling-policies/#headroom-attributes","text":"Headroom is a buffer of spare capacity (in terms of both memory and CPU) to ensure that cluster can meet suddenly increased demand for resources. It is based on the currently available total worker nodes resource capacity. For example, if headroom for memory and CPU are both set to 10%, and the cluster consists of 2 worker nodes equipped with 2 cores and 4GB RAM each, a total of 0.4 cores and 819MB would be considered as headroom in the next cluster size increase phase.","title":"Headroom attributes"},{"location":"guides/autoscaling-policies/#provisioning-decision","text":"After receiving the unschedulable pods event, the CAST AI recommendation engine will select the best price/performance ratio node capable of accommodating all of the currently unschedulable pods plus headroom. CAST AI will then provision it and join with the cluster. This process usually takes a few minutes, depending on the cloud service provider of your choice. Currently, only a single node will be added at a time. If any unschedulable pods still remain, the cycle is repeated until all the pods are scheduled (provided that the reason was insufficient resources).","title":"Provisioning decision"},{"location":"guides/autoscaling-policies/#configuring-the-unscheduled-pods-policy","text":"You can enable/disable the unschedulable pod's policy and set headroom settings either on the CAST AI console : or via the CAST AI policies API endpoint by setting values for \"unschedulablePods\" : { \"enabled\" : <value> , \"headroom\" : { \"cpuPercentage\" : <value> , \"memoryPercentage\" : <value> } } It may take a few minutes for the new settings to propagate.","title":"Configuring the unscheduled pod's policy"},{"location":"guides/autoscaling-policies/#policies-precedence-rules","text":"If multiple policies are enabled and multiple rules are triggered during the same evaluation period, they will be handled in the following order: Cluster CPU limits policy Horizontal Pod Autoscaler (HPA) policy Unscheduled pods policy Node deletion policy","title":"Policies precedence rules"},{"location":"guides/connect-to-node/","text":"Connect to node \u00b6 This guide describes how to connect to your cluster node via Kubernetes or native ssh. Connect via node-shell Kubernetes plugin \u00b6 With node-shell Kubernetes plugin you can connect into your node via Kubernetes API Server as a proxy. Install node-shell \u00b6 using krew : kubectl krew index add kvaps <a href = \"https://github.com/kvaps/krew-index\" >https://github.com/kvaps/krew-index</a> kubectl krew install kvaps/node-shell or using curl: curl -LO https://github.com/kvaps/kubectl-node-shell/raw/master/kubectl-node_shell chmod +x ./kubectl-node_shell sudo mv ./kubectl-node_shell /usr/local/bin/kubectl-node_shell Example node-shell usages \u00b6 # Get standard bash shell kubectl node-shell <node> # Execute custom command kubectl node-shell <node> -- echo 123 # Use stdin cat /etc/passwd | kubectl node-shell <node> -- sh -c 'cat > /tmp/passwd' # Run oneliner script kubectl node-shell <node> -- sh -c 'cat /tmp/passwd; rm -f /tmp/passwd' You need to be able to start privileged containers for that. Connect via Lens UI \u00b6 Lens is a great Kubernetes UI tool which has builtin functionality to connect into cluster node. Connect via native ssh \u00b6 With native SSH you can connect directly into your node without Kubernetes API. Install CAST CLI \u00b6 Install official CAST CLI Example CAST CLI usage \u00b6 cast -c=cluster-name node ssh my-node-name When to use native ssh with CAST CLI? Your Kubernetes cluster is not working properly (Kubernetes API Server is not accessible etc.) You need native SSH performance, eg: packet tracing with tcpdump etc. Kubernetes node-shell plugin spin ups a new pod with root access and proxies to Kubernetes API Server which is slower that direct SSH connection.","title":"Connect to node"},{"location":"guides/connect-to-node/#connect-to-node","text":"This guide describes how to connect to your cluster node via Kubernetes or native ssh.","title":"Connect to node"},{"location":"guides/connect-to-node/#connect-via-node-shell-kubernetes-plugin","text":"With node-shell Kubernetes plugin you can connect into your node via Kubernetes API Server as a proxy.","title":"Connect via node-shell Kubernetes plugin"},{"location":"guides/connect-to-node/#install-node-shell","text":"using krew : kubectl krew index add kvaps <a href = \"https://github.com/kvaps/krew-index\" >https://github.com/kvaps/krew-index</a> kubectl krew install kvaps/node-shell or using curl: curl -LO https://github.com/kvaps/kubectl-node-shell/raw/master/kubectl-node_shell chmod +x ./kubectl-node_shell sudo mv ./kubectl-node_shell /usr/local/bin/kubectl-node_shell","title":"Install node-shell"},{"location":"guides/connect-to-node/#example-node-shell-usages","text":"# Get standard bash shell kubectl node-shell <node> # Execute custom command kubectl node-shell <node> -- echo 123 # Use stdin cat /etc/passwd | kubectl node-shell <node> -- sh -c 'cat > /tmp/passwd' # Run oneliner script kubectl node-shell <node> -- sh -c 'cat /tmp/passwd; rm -f /tmp/passwd' You need to be able to start privileged containers for that.","title":"Example node-shell usages"},{"location":"guides/connect-to-node/#connect-via-lens-ui","text":"Lens is a great Kubernetes UI tool which has builtin functionality to connect into cluster node.","title":"Connect via Lens UI"},{"location":"guides/connect-to-node/#connect-via-native-ssh","text":"With native SSH you can connect directly into your node without Kubernetes API.","title":"Connect via native ssh"},{"location":"guides/connect-to-node/#install-cast-cli","text":"Install official CAST CLI","title":"Install CAST CLI"},{"location":"guides/connect-to-node/#example-cast-cli-usage","text":"cast -c=cluster-name node ssh my-node-name When to use native ssh with CAST CLI? Your Kubernetes cluster is not working properly (Kubernetes API Server is not accessible etc.) You need native SSH performance, eg: packet tracing with tcpdump etc. Kubernetes node-shell plugin spin ups a new pod with root access and proxies to Kubernetes API Server which is slower that direct SSH connection.","title":"Example CAST CLI usage"},{"location":"guides/hpa/","text":"Horizontal Pod Autoscaler \u00b6 Scaling an application \u00b6 You can scale an application in two ways: Vertically: by adding more resources (RAM/CPU/Disk IOPS) to the same instance, Horizontally: by adding more instances (replicas) of the same application. The problem with vertical scaling is that either the required hardware (RAM, CPU, Disk IOPS) in a single machine costs too much, or the cloud provider cannot provision a machine with enough resources. We use replica sets in Kubernetes to achieve horizontal scaling. The Horizontal Pod Autoscaler allows automating the process of maintaining the replica count proportionally to the application load. Horizontal scaling strategy \u00b6 The horizontal scaling strategy involves adding (or removing) the additional replicas of the same application. The problem here lies in the fact that most application load patterns can have spikes that are not predictable. This renders manual scaling nearly impossible. Luckily, we can automate this process. The HPA & KEDA \u00b6 Kubernetes has the Horizontal Pod Autoscaler (HPA) functionality. It can scale up (add more replicas) or down (remove idling replicas) based on some metrics. However, HPA does not have the metrics' source by default. CAST AI offers you a solution with the KEDA addon. How does it work \u00b6 KEDA consists of two components: operator - watches k8s for ScaledObject resources and configures HPA accordingly metrics-apiserver - a bridge between Kubernetes and various scaling sources (including Prometheus) These components configure Kubernetes HPA and set up the custom metric sources. This enables us to autoscale almost any workload: Deployment , ReplicaSet , ReplicationController , or StatefulSet . KEDA supports autoscaling Jobs as well. Enabling KEDA \u00b6 To be able to autoscale, you will need to enable KEDA addon on the Policies page: Navigate to an existing cluster (or see create cluster ). Go to the Policies menu. Enable the Horizontal pod autoscaler policy. Examples \u00b6 Autoscale Based on CPU and/or Memory usage \u00b6 Let's create a Deployment and a Service that we will Autoscale : apiVersion : apps/v1 kind : Deployment metadata : name : sample-app labels : app : sample-app spec : # Note that we omit the replica count so # when we redeploy, we wouldn't override # replica count set by the autoscaler #replicas: 1 selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app spec : containers : - image : luxas/autoscale-demo:v0.1.2 name : sample-app ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : sample-app labels : app : sample-app spec : ports : - port : 8080 name : http targetPort : 8080 protocol : TCP selector : app : sample-app Note : We do not specify the ReplicaCount ourselves Now set up a CPU-based Autoscaler apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : sample-app spec : scaleTargetRef : name : sample-app minReplicaCount : 1 # Optional. Default: 0 maxReplicaCount : 10 # Optional. Default: 100 triggers : # Either of the triggers can be omitted. - type : cpu metadata : # Possible values: `Value`, `Utilization`, or `AverageValue`. # More info at: https://keda.sh/docs/2.0/scalers/cpu/#trigger-specification type : \"Value\" value : \"30\" - type : memory metadata : # Possible values: `Value`, `Utilization`, or `AverageValue`. # More info at: https://keda.sh/docs/2.0/scalers/memory/ type : \"Value\" value : \"512\" Now our Deployment autoscaling will be triggered either by CPU or Memory usage. We could use any other trigger, or remove either of those if we want (i.e. to autoscale only on the CPU basis and remove the Memory trigger, or vice-versa). Autoscale based on the Prometheus metric \u00b6 It is possible to autoscale based on the result of an arbitrary Prometheus query. CAST AI k8s clusters come with Prometheus deployed out-of-the-box. Let's deploy the sample application again and instruct Prometheus to scrape metrics: apiVersion : apps/v1 kind : Deployment metadata : name : sample-app labels : app : sample-app spec : selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app annotations : # These annotations the main difference! prometheus.io/path : \"/metrics\" prometheus.io/port : \"8080\" prometheus.io/scrape : \"true\" spec : containers : - image : luxas/autoscale-demo:v0.1.2 name : sample-app ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : sample-app labels : app : sample-app spec : ports : - port : 8080 name : http targetPort : 8080 protocol : TCP selector : app : sample-app Now let's deploy the Autoscaler. apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : sample-app spec : scaleTargetRef : name : sample-app minReplicaCount : 1 # Optional. Default: 0 maxReplicaCount : 10 # Optional. Default: 100 triggers : - type : prometheus metadata : serverAddress : http://prom.castai:9090 metricName : http_requests_total_sample_app threshold : '1' # Note: query must return a vector/scalar single element response query : sum(rate(http_requests_total{app=\"sample-app\"}[2m])) Now let's generate some load and observe that the replica count is increased: # Deploy busybox image kubectl run -it --rm load-generator --image = busybox /bin/sh # Hit ENTER for command prompt # trigger infinite requests to the php-apache server while true ; do wget -q -O- http://sample-app:8080/metrics ; done # in order to cancel, hold CTRL+C # in order to quit, initiate CTRL+D sequence Troubleshooting \u00b6 Verify that KEDA is scheduled and running (the suffixes might be different): $ kubectl get pods -n keda NAME READY STATUS RESTARTS AGE keda-metrics-apiserver-59679c9f96-5lfr5 1 /1 Running 0 74m keda-operator-66744fc69d-7njdd 1 /1 Running 0 74m Describe ScaledObject for clues. In this case, scaledObjectRef points to nonexistent object: $ kubectl describe scaledobjects.keda.sh sample-app Name: sample-app Namespace: default Labels: scaledObjectName = sample-app Annotations: API Version: keda.sh/v1alpha1 Kind: ScaledObject Metadata: Creation Timestamp: 2020 -11-10T10:12:38Z Finalizers: finalizer.keda.sh Generation: 1 Managed Fields: <... snip ...> Resource Version: 394466 Self Link: /apis/keda.sh/v1alpha1/namespaces/default/scaledobjects/sample-app UID: 9394d57a-ae66-4e80-baf4-8d6bb7fd36f9 Spec: Advanced: Horizontal Pod Autoscaler Config: Behavior: Scale Down: Policies: Period Seconds: 15 Type: Percent Value: 100 Stabilization Window Seconds: 300 Restore To Original Replica Count: true Cooldown Period: 300 Max Replica Count: 10 Min Replica Count: 1 Polling Interval: 30 Scale Target Ref: API Version: apps/v1 Kind: Deployment Name: sample-app Triggers: Metadata: Metric Name: http_requests_total Query: sum ( rate ( http_requests_total { app = \"sample-app\" }[ 2m ])) Server Address: http://prom.castai:9090 Threshold: 1 Type: prometheus Status: Conditions: Message: ScaledObject doesn 't have correct scaleTargetRef specification Reason: ScaledObjectCheckFailed Status: False <--------- This means that this check didn' t pass Type: Ready Message: ScaledObject check failed Reason: UnkownState Status: Unknown Type: Active Events: <none> Inspect KEDA operator logs: kubectl logs -n keda $( kubectl get pods -n keda -o name | grep operator )","title":"Horizontal Pod Autoscaler"},{"location":"guides/hpa/#horizontal-pod-autoscaler","text":"","title":"Horizontal Pod Autoscaler"},{"location":"guides/hpa/#scaling-an-application","text":"You can scale an application in two ways: Vertically: by adding more resources (RAM/CPU/Disk IOPS) to the same instance, Horizontally: by adding more instances (replicas) of the same application. The problem with vertical scaling is that either the required hardware (RAM, CPU, Disk IOPS) in a single machine costs too much, or the cloud provider cannot provision a machine with enough resources. We use replica sets in Kubernetes to achieve horizontal scaling. The Horizontal Pod Autoscaler allows automating the process of maintaining the replica count proportionally to the application load.","title":"Scaling an application"},{"location":"guides/hpa/#horizontal-scaling-strategy","text":"The horizontal scaling strategy involves adding (or removing) the additional replicas of the same application. The problem here lies in the fact that most application load patterns can have spikes that are not predictable. This renders manual scaling nearly impossible. Luckily, we can automate this process.","title":"Horizontal scaling strategy"},{"location":"guides/hpa/#the-hpa-keda","text":"Kubernetes has the Horizontal Pod Autoscaler (HPA) functionality. It can scale up (add more replicas) or down (remove idling replicas) based on some metrics. However, HPA does not have the metrics' source by default. CAST AI offers you a solution with the KEDA addon.","title":"The HPA &amp; KEDA"},{"location":"guides/hpa/#how-does-it-work","text":"KEDA consists of two components: operator - watches k8s for ScaledObject resources and configures HPA accordingly metrics-apiserver - a bridge between Kubernetes and various scaling sources (including Prometheus) These components configure Kubernetes HPA and set up the custom metric sources. This enables us to autoscale almost any workload: Deployment , ReplicaSet , ReplicationController , or StatefulSet . KEDA supports autoscaling Jobs as well.","title":"How does it work"},{"location":"guides/hpa/#enabling-keda","text":"To be able to autoscale, you will need to enable KEDA addon on the Policies page: Navigate to an existing cluster (or see create cluster ). Go to the Policies menu. Enable the Horizontal pod autoscaler policy.","title":"Enabling KEDA"},{"location":"guides/hpa/#examples","text":"","title":"Examples"},{"location":"guides/hpa/#autoscale-based-on-cpu-andor-memory-usage","text":"Let's create a Deployment and a Service that we will Autoscale : apiVersion : apps/v1 kind : Deployment metadata : name : sample-app labels : app : sample-app spec : # Note that we omit the replica count so # when we redeploy, we wouldn't override # replica count set by the autoscaler #replicas: 1 selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app spec : containers : - image : luxas/autoscale-demo:v0.1.2 name : sample-app ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : sample-app labels : app : sample-app spec : ports : - port : 8080 name : http targetPort : 8080 protocol : TCP selector : app : sample-app Note : We do not specify the ReplicaCount ourselves Now set up a CPU-based Autoscaler apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : sample-app spec : scaleTargetRef : name : sample-app minReplicaCount : 1 # Optional. Default: 0 maxReplicaCount : 10 # Optional. Default: 100 triggers : # Either of the triggers can be omitted. - type : cpu metadata : # Possible values: `Value`, `Utilization`, or `AverageValue`. # More info at: https://keda.sh/docs/2.0/scalers/cpu/#trigger-specification type : \"Value\" value : \"30\" - type : memory metadata : # Possible values: `Value`, `Utilization`, or `AverageValue`. # More info at: https://keda.sh/docs/2.0/scalers/memory/ type : \"Value\" value : \"512\" Now our Deployment autoscaling will be triggered either by CPU or Memory usage. We could use any other trigger, or remove either of those if we want (i.e. to autoscale only on the CPU basis and remove the Memory trigger, or vice-versa).","title":"Autoscale Based on CPU and/or Memory usage"},{"location":"guides/hpa/#autoscale-based-on-the-prometheus-metric","text":"It is possible to autoscale based on the result of an arbitrary Prometheus query. CAST AI k8s clusters come with Prometheus deployed out-of-the-box. Let's deploy the sample application again and instruct Prometheus to scrape metrics: apiVersion : apps/v1 kind : Deployment metadata : name : sample-app labels : app : sample-app spec : selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app annotations : # These annotations the main difference! prometheus.io/path : \"/metrics\" prometheus.io/port : \"8080\" prometheus.io/scrape : \"true\" spec : containers : - image : luxas/autoscale-demo:v0.1.2 name : sample-app ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : sample-app labels : app : sample-app spec : ports : - port : 8080 name : http targetPort : 8080 protocol : TCP selector : app : sample-app Now let's deploy the Autoscaler. apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : sample-app spec : scaleTargetRef : name : sample-app minReplicaCount : 1 # Optional. Default: 0 maxReplicaCount : 10 # Optional. Default: 100 triggers : - type : prometheus metadata : serverAddress : http://prom.castai:9090 metricName : http_requests_total_sample_app threshold : '1' # Note: query must return a vector/scalar single element response query : sum(rate(http_requests_total{app=\"sample-app\"}[2m])) Now let's generate some load and observe that the replica count is increased: # Deploy busybox image kubectl run -it --rm load-generator --image = busybox /bin/sh # Hit ENTER for command prompt # trigger infinite requests to the php-apache server while true ; do wget -q -O- http://sample-app:8080/metrics ; done # in order to cancel, hold CTRL+C # in order to quit, initiate CTRL+D sequence","title":"Autoscale based on the Prometheus metric"},{"location":"guides/hpa/#troubleshooting","text":"Verify that KEDA is scheduled and running (the suffixes might be different): $ kubectl get pods -n keda NAME READY STATUS RESTARTS AGE keda-metrics-apiserver-59679c9f96-5lfr5 1 /1 Running 0 74m keda-operator-66744fc69d-7njdd 1 /1 Running 0 74m Describe ScaledObject for clues. In this case, scaledObjectRef points to nonexistent object: $ kubectl describe scaledobjects.keda.sh sample-app Name: sample-app Namespace: default Labels: scaledObjectName = sample-app Annotations: API Version: keda.sh/v1alpha1 Kind: ScaledObject Metadata: Creation Timestamp: 2020 -11-10T10:12:38Z Finalizers: finalizer.keda.sh Generation: 1 Managed Fields: <... snip ...> Resource Version: 394466 Self Link: /apis/keda.sh/v1alpha1/namespaces/default/scaledobjects/sample-app UID: 9394d57a-ae66-4e80-baf4-8d6bb7fd36f9 Spec: Advanced: Horizontal Pod Autoscaler Config: Behavior: Scale Down: Policies: Period Seconds: 15 Type: Percent Value: 100 Stabilization Window Seconds: 300 Restore To Original Replica Count: true Cooldown Period: 300 Max Replica Count: 10 Min Replica Count: 1 Polling Interval: 30 Scale Target Ref: API Version: apps/v1 Kind: Deployment Name: sample-app Triggers: Metadata: Metric Name: http_requests_total Query: sum ( rate ( http_requests_total { app = \"sample-app\" }[ 2m ])) Server Address: http://prom.castai:9090 Threshold: 1 Type: prometheus Status: Conditions: Message: ScaledObject doesn 't have correct scaleTargetRef specification Reason: ScaledObjectCheckFailed Status: False <--------- This means that this check didn' t pass Type: Ready Message: ScaledObject check failed Reason: UnkownState Status: Unknown Type: Active Events: <none> Inspect KEDA operator logs: kubectl logs -n keda $( kubectl get pods -n keda -o name | grep operator )","title":"Troubleshooting"},{"location":"guides/ingress/","text":"Exposing your app to the internet \u00b6 To have your CAST AI hosted application available on the internet you will need to deploy an Ingress. Kubernetes documentation - Ingress CAST AI clusters are automatically provisioned with: Ingress controller and the necessary multi-cloud load balancers infrastructure; A certificate manager configured to manage TLS certificates with letsencrypt.org ; Metric collection for your Ingress traffic; See cluster infrastructure for more details. Let's deploy, configure, and inspect a basic application: an empty Caddy server. Prerequisites \u00b6 CAST AI cluster - see create cluster . GSLB DNS value of the cluster - you will find this in /clusters details page. This will be an internal DNS name for your Ingress. CNAME alias for TLS setup - use a hostname of your choice and create a CNAME record with GSLB DNS value. Example if: GSLB DNS value 1234567890.your-cluster-name-7da6f229.onmulti.cloud Hostname https://sample-app.yourdomain.com Then: CNAME name CNAME value sample-app 1234567890.your-cluster-name-7da6f229.onmulti.cloud Check the DNS resolution (e.g. dig sample-app.yourdomain.com ) to see that the name resolves to one or more cloud-specific load balancers. Deployment \u00b6 This is a basic setup consisting of 2-replica deployment, a service description for it, and an Ingress resource to publish that service. Change value sample-app.yourdomain.com to the DNS CNAME that you have created, and deploy everything else as-is to your cluster. apiVersion : v1 kind : Namespace metadata : name : sample-app --- apiVersion : apps/v1 kind : Deployment metadata : name : sample-app namespace : sample-app spec : replicas : 2 selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app spec : containers : - name : sample-app image : caddy:2.2.1-alpine ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : sample-app namespace : sample-app spec : type : NodePort selector : app : sample-app ports : - name : http port : 80 targetPort : 80 --- apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress namespace : sample-app annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : - sample-app.yourdomain.com secretName : sample-app rules : - host : sample-app.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http Verification \u00b6 Once you deploy the configuration above, the application will be ready for testing in a few moments. Check in the browser or CLI, e.g.: $ curl -L -I sample-app.yourdomain.com HTTP/1.1 308 Permanent Redirect Date: Wed, 13 Jan 2021 11:30:52 GMT Content-Type: text/HTML Content-Length: 164 Connection: keep-alive Location: https://sample-app.yourdomain.com/ HTTP/2 200 date: Wed, 13 Jan 2021 11:30:52 GMT content-type: text/html; charset=utf-8 content-length: 12226 vary: Accept-Encoding accept-ranges: bytes etag: \"qlhhn49fm\" last-modified: Thu, 17 Dec 2020 12:35:28 GMT strict-transport-security: max-age=15724800; includeSubDomains You can see that: HTTP->HTTPS redirect is established automatically; Once redirected to HTTPS, your application TLS setup works properly (curl is able to verify certificate validity for your domain). Deployment without CNAME alias \u00b6 If you skipped the DNS setup, you will still be able to ping your application and get a response back. The only difference is that the TLS certificate will not be provisioned, as the certificate manager cannot complete an HTTP-01 challenge without LetsEncrypt being able to reach your app via the \"official\" URL. To ping the application without a DNS CNAME, use the internal DNS name and pass the \"host\" header for the Ingress routing to work. You will need to ignore certificate errors, as your application will be using a self-signed certificate as a fallback. $ curl -s -k -H \"Host: sample-app.yourdomain.com\" https://1234567890.your-cluster-name-7da6f229.onmulti.cloud | head -n 4 <!DOCTYPE html> <html> <head> <title>Caddy works!</title> If you do not intend to create a user-friendly URL, another alternative is to use an internal DNS name as an Ingress host. This will enable the certificate manager to provision a proper TLS certificate and your application will be reachable via this name directly. spec : tls : - hosts : - 1234567890.your-cluster-name-7da6f229.onmulti.cloud secretName : sample-app rules : - host : 1234567890.your-cluster-name-7da6f229.onmulti.cloud http : Metrics \u00b6 Once the application is up and running you can check the Ingress metrics and dashboard. Go to CAST.AI console /clusters details page and click on the \"Grafana logs\" link in the side menu. Once in Grafana, click \"Home\" in the top-left corner and open the \"NGINX Ingress controller\" dashboard. You will see something similar to this: This dashboard provides an overview of your application traffic. To tailor the dashboard to your specific needs, refer to NGINX metrics documentation for more details on available metrics. Combinations \u00b6 Single host, multiple services \u00b6 You can use path-based routing to redirect traffic to specific services using Ingress rule paths: Kubernetes documentation - Ingress path types spec : rules : - host : sample-app.yourdomain.com http : paths : - path : /static backend : serviceName : static-resources servicePort : http - path : / backend : serviceName : base-app servicePort : http Multiple hosts \u00b6 To manage multiple domains, you can deploy multiple Ingress resources, or include more domains into the same Ingress resource. # first host apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress namespace : sample-app annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : - sample-app.yourdomain.com secretName : sample-app-cert rules : - host : sample-app.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http --- # second host apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress2 namespace : sample-app annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : - sample-app2.yourdomain.com # note that secret needs to be unique for each domain, unless deployments # will be separated by kubernetes namespaces secretName : sample-app-cert-2 rules : - host : sample-app2.yourdomain.com http : paths : - path : / backend : serviceName : sample-app2 servicePort : http --- # combining: multiple hosts per certificate and/or multiple certificates per single ingress resource apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress3 namespace : sample-app annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : # two domains under a single certificate - sample-app3.yourdomain.com - sample-app3-alternative.yourdomain.com secretName : sample-app3-cert - hosts : # another side-by-side certificate - sample-app4.yourdomain.com secretName : sample-app4-cert rules : - host : sample-app3.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http - host : sample-app3-alternative.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http - host : sample-app4.yourdomain.com http : paths : - path : / backend : serviceName : sample-app2 servicePort : http Troubleshooting \u00b6 1. Check that ingress controller is properly initialized \u00b6 kubectl -n ingress-bundle get svc ingress-bundle-ingress-nginx-controller It should show at least two EXTERNAL-IP records, eg: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-bundle-ingress-nginx-controller LoadBalancer 10.96.192.165 1595234145.am-f46bb49a.local.onmulti.cloud,34.89.152.59 80:30562/TCP,443:31333/TCP 4m52s If EXTERNAL-IP shown as pending check service-operator logs which is responsible for creating load balancers. kubectl -n kube-system logs -f -l app = service-operator 2. Inspect application ingress status \u00b6 kubectl -n sample-app describe ingress sample-app-ingress Events should contain CreateCertificate and Sync events. 3. Inspect certificate \u00b6 kubectl -n sample-app get certificate If it's status shown as False check cert-manager logs. kubectl -n ingress-bundle logs -f -l app = cert-manager","title":"Ingress"},{"location":"guides/ingress/#exposing-your-app-to-the-internet","text":"To have your CAST AI hosted application available on the internet you will need to deploy an Ingress. Kubernetes documentation - Ingress CAST AI clusters are automatically provisioned with: Ingress controller and the necessary multi-cloud load balancers infrastructure; A certificate manager configured to manage TLS certificates with letsencrypt.org ; Metric collection for your Ingress traffic; See cluster infrastructure for more details. Let's deploy, configure, and inspect a basic application: an empty Caddy server.","title":"Exposing your app to the internet"},{"location":"guides/ingress/#prerequisites","text":"CAST AI cluster - see create cluster . GSLB DNS value of the cluster - you will find this in /clusters details page. This will be an internal DNS name for your Ingress. CNAME alias for TLS setup - use a hostname of your choice and create a CNAME record with GSLB DNS value. Example if: GSLB DNS value 1234567890.your-cluster-name-7da6f229.onmulti.cloud Hostname https://sample-app.yourdomain.com Then: CNAME name CNAME value sample-app 1234567890.your-cluster-name-7da6f229.onmulti.cloud Check the DNS resolution (e.g. dig sample-app.yourdomain.com ) to see that the name resolves to one or more cloud-specific load balancers.","title":"Prerequisites"},{"location":"guides/ingress/#deployment","text":"This is a basic setup consisting of 2-replica deployment, a service description for it, and an Ingress resource to publish that service. Change value sample-app.yourdomain.com to the DNS CNAME that you have created, and deploy everything else as-is to your cluster. apiVersion : v1 kind : Namespace metadata : name : sample-app --- apiVersion : apps/v1 kind : Deployment metadata : name : sample-app namespace : sample-app spec : replicas : 2 selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app spec : containers : - name : sample-app image : caddy:2.2.1-alpine ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : sample-app namespace : sample-app spec : type : NodePort selector : app : sample-app ports : - name : http port : 80 targetPort : 80 --- apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress namespace : sample-app annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : - sample-app.yourdomain.com secretName : sample-app rules : - host : sample-app.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http","title":"Deployment"},{"location":"guides/ingress/#verification","text":"Once you deploy the configuration above, the application will be ready for testing in a few moments. Check in the browser or CLI, e.g.: $ curl -L -I sample-app.yourdomain.com HTTP/1.1 308 Permanent Redirect Date: Wed, 13 Jan 2021 11:30:52 GMT Content-Type: text/HTML Content-Length: 164 Connection: keep-alive Location: https://sample-app.yourdomain.com/ HTTP/2 200 date: Wed, 13 Jan 2021 11:30:52 GMT content-type: text/html; charset=utf-8 content-length: 12226 vary: Accept-Encoding accept-ranges: bytes etag: \"qlhhn49fm\" last-modified: Thu, 17 Dec 2020 12:35:28 GMT strict-transport-security: max-age=15724800; includeSubDomains You can see that: HTTP->HTTPS redirect is established automatically; Once redirected to HTTPS, your application TLS setup works properly (curl is able to verify certificate validity for your domain).","title":"Verification"},{"location":"guides/ingress/#deployment-without-cname-alias","text":"If you skipped the DNS setup, you will still be able to ping your application and get a response back. The only difference is that the TLS certificate will not be provisioned, as the certificate manager cannot complete an HTTP-01 challenge without LetsEncrypt being able to reach your app via the \"official\" URL. To ping the application without a DNS CNAME, use the internal DNS name and pass the \"host\" header for the Ingress routing to work. You will need to ignore certificate errors, as your application will be using a self-signed certificate as a fallback. $ curl -s -k -H \"Host: sample-app.yourdomain.com\" https://1234567890.your-cluster-name-7da6f229.onmulti.cloud | head -n 4 <!DOCTYPE html> <html> <head> <title>Caddy works!</title> If you do not intend to create a user-friendly URL, another alternative is to use an internal DNS name as an Ingress host. This will enable the certificate manager to provision a proper TLS certificate and your application will be reachable via this name directly. spec : tls : - hosts : - 1234567890.your-cluster-name-7da6f229.onmulti.cloud secretName : sample-app rules : - host : 1234567890.your-cluster-name-7da6f229.onmulti.cloud http :","title":"Deployment without CNAME alias"},{"location":"guides/ingress/#metrics","text":"Once the application is up and running you can check the Ingress metrics and dashboard. Go to CAST.AI console /clusters details page and click on the \"Grafana logs\" link in the side menu. Once in Grafana, click \"Home\" in the top-left corner and open the \"NGINX Ingress controller\" dashboard. You will see something similar to this: This dashboard provides an overview of your application traffic. To tailor the dashboard to your specific needs, refer to NGINX metrics documentation for more details on available metrics.","title":"Metrics"},{"location":"guides/ingress/#combinations","text":"","title":"Combinations"},{"location":"guides/ingress/#single-host-multiple-services","text":"You can use path-based routing to redirect traffic to specific services using Ingress rule paths: Kubernetes documentation - Ingress path types spec : rules : - host : sample-app.yourdomain.com http : paths : - path : /static backend : serviceName : static-resources servicePort : http - path : / backend : serviceName : base-app servicePort : http","title":"Single host, multiple services"},{"location":"guides/ingress/#multiple-hosts","text":"To manage multiple domains, you can deploy multiple Ingress resources, or include more domains into the same Ingress resource. # first host apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress namespace : sample-app annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : - sample-app.yourdomain.com secretName : sample-app-cert rules : - host : sample-app.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http --- # second host apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress2 namespace : sample-app annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : - sample-app2.yourdomain.com # note that secret needs to be unique for each domain, unless deployments # will be separated by kubernetes namespaces secretName : sample-app-cert-2 rules : - host : sample-app2.yourdomain.com http : paths : - path : / backend : serviceName : sample-app2 servicePort : http --- # combining: multiple hosts per certificate and/or multiple certificates per single ingress resource apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress3 namespace : sample-app annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : # two domains under a single certificate - sample-app3.yourdomain.com - sample-app3-alternative.yourdomain.com secretName : sample-app3-cert - hosts : # another side-by-side certificate - sample-app4.yourdomain.com secretName : sample-app4-cert rules : - host : sample-app3.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http - host : sample-app3-alternative.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http - host : sample-app4.yourdomain.com http : paths : - path : / backend : serviceName : sample-app2 servicePort : http","title":"Multiple hosts"},{"location":"guides/ingress/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"guides/ingress/#1-check-that-ingress-controller-is-properly-initialized","text":"kubectl -n ingress-bundle get svc ingress-bundle-ingress-nginx-controller It should show at least two EXTERNAL-IP records, eg: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-bundle-ingress-nginx-controller LoadBalancer 10.96.192.165 1595234145.am-f46bb49a.local.onmulti.cloud,34.89.152.59 80:30562/TCP,443:31333/TCP 4m52s If EXTERNAL-IP shown as pending check service-operator logs which is responsible for creating load balancers. kubectl -n kube-system logs -f -l app = service-operator","title":"1. Check that ingress controller is properly initialized"},{"location":"guides/ingress/#2-inspect-application-ingress-status","text":"kubectl -n sample-app describe ingress sample-app-ingress Events should contain CreateCertificate and Sync events.","title":"2. Inspect application ingress status"},{"location":"guides/ingress/#3-inspect-certificate","text":"kubectl -n sample-app get certificate If it's status shown as False check cert-manager logs. kubectl -n ingress-bundle logs -f -l app = cert-manager","title":"3. Inspect certificate"},{"location":"guides/pod-pinning/","text":"Configure pod placement by topology \u00b6 This guide will show how to place pods only on a particular cloud or clouds. Kubernetes supports this by using: nodeSelector nodeAffinity/Anti-Affinity topologySpreadConstraints All of these methods require special labels to be present on each Kubernetes node. CAST AI multi cloud Kubernetes cluster nodes are already equipped with the following labels: Label Type Description Example(s) node.kubernetes.io/instance-type well-known Node type (cloud-specific) t3a.large, e2-standard-4 kubernetes.io/arch well-known Node CPU architecture amd64 kubernetes.io/hostname well-known Node Hostname ip-10-10-2-81, testcluster-31qd-gcp-3ead kubernetes.io/os well-known Node Operating System linux topology.kubernetes.io/region well-known Node region in the CSP eu-central-1 topology.kubernetes.io/zone well-known Node zone of the region in the CSP eu-central-1a topology.cast.ai/csp cast-specific Node Cloud Service Provider aws, gcp, azure How to pin a pod to AWS \u00b6 We will use affinity.nodeAffinity : affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.cast.ai/csp operator: In values: - aws Pod example: apiVersion: v1 kind: Pod metadata: name: nginx-pod labels: app: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.storage.csi.cast.ai/csp operator: In values: - aws containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web StatefulSet example, it will create 3 pods each in every cloud (note the podAntiAffinity) apiVersion: apps/v1 kind: StatefulSet metadata: name: a-web spec: podManagementPolicy: Parallel serviceName: \"nginx\" replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.storage.csi.cast.ai/csp operator: In values: - aws - gcp - azure podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: topology.storage.csi.cast.ai/csp containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi","title":"Pod placement"},{"location":"guides/pod-pinning/#configure-pod-placement-by-topology","text":"This guide will show how to place pods only on a particular cloud or clouds. Kubernetes supports this by using: nodeSelector nodeAffinity/Anti-Affinity topologySpreadConstraints All of these methods require special labels to be present on each Kubernetes node. CAST AI multi cloud Kubernetes cluster nodes are already equipped with the following labels: Label Type Description Example(s) node.kubernetes.io/instance-type well-known Node type (cloud-specific) t3a.large, e2-standard-4 kubernetes.io/arch well-known Node CPU architecture amd64 kubernetes.io/hostname well-known Node Hostname ip-10-10-2-81, testcluster-31qd-gcp-3ead kubernetes.io/os well-known Node Operating System linux topology.kubernetes.io/region well-known Node region in the CSP eu-central-1 topology.kubernetes.io/zone well-known Node zone of the region in the CSP eu-central-1a topology.cast.ai/csp cast-specific Node Cloud Service Provider aws, gcp, azure","title":"Configure pod placement by topology"},{"location":"guides/pod-pinning/#how-to-pin-a-pod-to-aws","text":"We will use affinity.nodeAffinity : affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.cast.ai/csp operator: In values: - aws Pod example: apiVersion: v1 kind: Pod metadata: name: nginx-pod labels: app: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.storage.csi.cast.ai/csp operator: In values: - aws containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web StatefulSet example, it will create 3 pods each in every cloud (note the podAntiAffinity) apiVersion: apps/v1 kind: StatefulSet metadata: name: a-web spec: podManagementPolicy: Parallel serviceName: \"nginx\" replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.storage.csi.cast.ai/csp operator: In values: - aws - gcp - azure podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: topology.storage.csi.cast.ai/csp containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi","title":"How to pin a pod to AWS"},{"location":"guides/spot/","text":"Spot/Preemptible Instances \u00b6 The CAST AI autoscaler supports running your workloads on Spot/Preemptible instances. This guide will help you configure and run it in 5 minutes. Available configurations \u00b6 Tolerations \u00b6 When to use: spot instances are optional When a pod is marked only with tolerations , the Kubernetes scheduler could place such a pod/pods on regular nodes as well. ... tolerations : - key : scheduling.cast.ai/spot operator : Exists ... Node Selectors \u00b6 When to use: only use spot instances If you want to make sure that a pod is scheduled on spot instances only, add nodeSelector as well as per the example below. The autoscaler will then ensure that only a spot instance is picked whenever your pod requires additional workload in the cluster. ... tolerations : - key : scheduling.cast.ai/spot operator : Exists nodeSelector : scheduling.cast.ai/spot : \"true\" ... Step-by-step deployment on Spot Instance \u00b6 In this step-by-step guide, we demonstrate how to use Spot Instances with your CAST AI clusters. To do that, we will use an example NGINX deployment configured to run only on Spot/Preemptible instances. 0. Pre-requisites \u00b6 CAST AI cluster - see create cluster . Kubeconfig file - see deploy application 1. Enable relevant policies \u00b6 To start using Spot instances autoscaler enable the following policies under the Policies menu in the UI: Spot/Preemptible instances policy This policy allows the autoscaler to use spot instances Unschedulable pods policy This policy requests an additional workload to be scheduled based on your deployment requirements (i.e. run on spot instances) 2. Example deployment \u00b6 Save the following yaml file, and name it: nginx.yaml : apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : nodeSelector : scheduling.cast.ai/spot : \"true\" tolerations : - key : scheduling.cast.ai/spot operator : Exists containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 resources : requests : cpu : '2' limits : cpu : '3' 2.1. Apply the example deployment \u00b6 With kubeconfig set in your current shell session, you can execute the following (or use other means of applying deployment files): kubectl apply -f ngninx.yaml 2.2. Wait several minutes \u00b6 Once the deployment is created, it will take up to several minutes for the autoscaler to pick up the information about your pending deployment and schedule the relevant workloads in order to satisfy the deployment needs, such as: This deployment tolerates spot instances This deployment must run only on spot instances 3. Spot Instance added \u00b6 You can see your newly added spot instance in the cluster node list. 3.1. AWS instance list \u00b6 Just to double-check, go to the AWS console and check that the added node has the Lifecycle: spot indicator.","title":"Spot/Preemptible Instances"},{"location":"guides/spot/#spotpreemptible-instances","text":"The CAST AI autoscaler supports running your workloads on Spot/Preemptible instances. This guide will help you configure and run it in 5 minutes.","title":"Spot/Preemptible Instances"},{"location":"guides/spot/#available-configurations","text":"","title":"Available configurations"},{"location":"guides/spot/#tolerations","text":"When to use: spot instances are optional When a pod is marked only with tolerations , the Kubernetes scheduler could place such a pod/pods on regular nodes as well. ... tolerations : - key : scheduling.cast.ai/spot operator : Exists ...","title":"Tolerations"},{"location":"guides/spot/#node-selectors","text":"When to use: only use spot instances If you want to make sure that a pod is scheduled on spot instances only, add nodeSelector as well as per the example below. The autoscaler will then ensure that only a spot instance is picked whenever your pod requires additional workload in the cluster. ... tolerations : - key : scheduling.cast.ai/spot operator : Exists nodeSelector : scheduling.cast.ai/spot : \"true\" ...","title":"Node Selectors"},{"location":"guides/spot/#step-by-step-deployment-on-spot-instance","text":"In this step-by-step guide, we demonstrate how to use Spot Instances with your CAST AI clusters. To do that, we will use an example NGINX deployment configured to run only on Spot/Preemptible instances.","title":"Step-by-step deployment on Spot Instance"},{"location":"guides/spot/#0-pre-requisites","text":"CAST AI cluster - see create cluster . Kubeconfig file - see deploy application","title":"0. Pre-requisites"},{"location":"guides/spot/#1-enable-relevant-policies","text":"To start using Spot instances autoscaler enable the following policies under the Policies menu in the UI: Spot/Preemptible instances policy This policy allows the autoscaler to use spot instances Unschedulable pods policy This policy requests an additional workload to be scheduled based on your deployment requirements (i.e. run on spot instances)","title":"1. Enable relevant policies"},{"location":"guides/spot/#2-example-deployment","text":"Save the following yaml file, and name it: nginx.yaml : apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : nodeSelector : scheduling.cast.ai/spot : \"true\" tolerations : - key : scheduling.cast.ai/spot operator : Exists containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 resources : requests : cpu : '2' limits : cpu : '3'","title":"2. Example deployment"},{"location":"guides/spot/#21-apply-the-example-deployment","text":"With kubeconfig set in your current shell session, you can execute the following (or use other means of applying deployment files): kubectl apply -f ngninx.yaml","title":"2.1. Apply the example deployment"},{"location":"guides/spot/#22-wait-several-minutes","text":"Once the deployment is created, it will take up to several minutes for the autoscaler to pick up the information about your pending deployment and schedule the relevant workloads in order to satisfy the deployment needs, such as: This deployment tolerates spot instances This deployment must run only on spot instances","title":"2.2. Wait several minutes"},{"location":"guides/spot/#3-spot-instance-added","text":"You can see your newly added spot instance in the cluster node list.","title":"3. Spot Instance added"},{"location":"guides/spot/#31-aws-instance-list","text":"Just to double-check, go to the AWS console and check that the added node has the Lifecycle: spot indicator.","title":"3.1. AWS instance list"},{"location":"guides/start-saving-quickly/","text":"Start saving on your connected EKS cluster immediately \u00b6 So, you liked the results of the Savings Estimator after connecting your existing EKS cluster to CAST AI, but don't want to wait until these savings reach you in a slow and risk-free ongoing process. You can speed this up, here's how. Register your connected cluster (EKS) \u00b6 You need to register first - which means creating an IAM user for CAST AI to optimize your cluster. Enable policies \u00b6 Enabled Node deletion policy - this policy will remove nodes without pods (ignores DaemonSets). Enable Unscheduled Pod policy - it will make sure that you always have the capacity in the cluster to run pods. The Unscheduled Pod policy will provision a new node when required, taking no more than 2-3 minutes. Adjust headroom % for migration purposes - each node adds overhead through DaemonSets, also more smaller nodes means that more pods won't find their destination on the same node (added latency). So ideally, one should have nodes that are as large as possible, but 5-6 nodes minimum (for below 200 CPUs cluster) for good SLA and adequate capacity distribution for the lifecycle process (upgrades, patching). Take the number from Available Savings - this is the total amount of nodes you should have in the optimized state. headroom percentage = 100 / Amount_of_Nodes_in_suggested_optimized_state+1 In the Policies tab, it should look like this: \"Slow and safe\" or \"maximize savings now\" \u00b6 Evictor is our recommended way - it will constantly look for inefficiencies. But reducing costs in a safe manner takes time. If you want to maximize your savings as quickly as possible and you have a maintenance window, you can do it in CAST AI. Install Evictor (continuous improvements) \u00b6 Evictor will compact your pods into fewer nodes, creating empty nodes that will be removed by the Node deletion policy: helm repo add castai https://castai.github.io/official-addons helm -n kube-system upgrade -i evictor castai/evictor --set dryRun=false This process will take some time. Also, Evictor will not cause any downtime to single replica deployments / StatefulSets, pods without ReplicaSet, meaning that those nodes can't be removed gracefully. Stir the pod with manual migration \u00b6 You will have to get rid of your existing nodes and let CAST AI create an optimized state right away. This might cause some downtime depending on your workload configuration. For example, pick 50% of your nodes in one availability zone (AZ) or 20% of nodes if your connected cluster is in a single AZ. kubectl get nodes -Lfailure-domain.beta.kubernetes.io/zone --selector=eks.amazonaws.com/nodegroup-image The percentage is arbitrary - it depends on your risk appetite and how much time you want to spend on this. Taint (cordon) the selected nodes, so no new pods are placed on these nodes. We like Lens k8s ide, but you can use kubectl as well: kubectl cordon nodeName1 kubectl cordon nodeName2 And now drain these nodes: kubectl drain nodeName1 --ignore-daemonsets --delete-local-data kubectl drain nodeName2 --ignore-daemonsets --delete-local-data Some nodes will not drain because of the Disruption Budget violation (downtime). These cases should be fixed since they are going to cause pain in the future (or at least noted to be addressed when most convenient). If you want to progress anyway and accept downtime, cancel the drain command and retry draining with the additional --force flag. You should see that the drained nodes disappear (empty Node deletion policy) and, in few moments, new nodes in the same availability zone appear (Unscheduled Pod policy with Headroom). Check the remaining nodes. You will see that list is shorter because the command below selects only nodes in the AWS autoscaling group (ASG) and new nodes don't use ASG. kubectl get nodes -Lfailure-domain.beta.kubernetes.io/zone --selector=eks.amazonaws.com/nodegroup-image Select next batch -> cordon -> drain -> write down problematic pod that don't migrate easily -> rinse and repeat until the list is empty. Utilize Spot instances \u00b6 In the Available savings window, you can find a list of deployments that could use Spot instances. I have a recommendation service running with 10 replicas. I could separate this workload into two deployments: Reduce the current replica count to a bare minimum (in my case, 2 replicas), Create a copy of deployment with \"-spot\" appending name, add toleration, and set to 8 replicas - or beter, configure to use KEDA, see HPA documentation ... tolerations : - key : scheduling.cast.ai/spot operator : Exists ... You're all done \u00b6 Share the Available savings window screenshot with your CFO/manager - there's nothing left to save. Reduce the Headroom policy to a smaller number that fits your smooth organic growth better. Install Evictor, if you haven't already done that.","title":"Start saving quickly"},{"location":"guides/start-saving-quickly/#start-saving-on-your-connected-eks-cluster-immediately","text":"So, you liked the results of the Savings Estimator after connecting your existing EKS cluster to CAST AI, but don't want to wait until these savings reach you in a slow and risk-free ongoing process. You can speed this up, here's how.","title":"Start saving on your connected EKS cluster immediately"},{"location":"guides/start-saving-quickly/#register-your-connected-cluster-eks","text":"You need to register first - which means creating an IAM user for CAST AI to optimize your cluster.","title":"Register your connected cluster (EKS)"},{"location":"guides/start-saving-quickly/#enable-policies","text":"Enabled Node deletion policy - this policy will remove nodes without pods (ignores DaemonSets). Enable Unscheduled Pod policy - it will make sure that you always have the capacity in the cluster to run pods. The Unscheduled Pod policy will provision a new node when required, taking no more than 2-3 minutes. Adjust headroom % for migration purposes - each node adds overhead through DaemonSets, also more smaller nodes means that more pods won't find their destination on the same node (added latency). So ideally, one should have nodes that are as large as possible, but 5-6 nodes minimum (for below 200 CPUs cluster) for good SLA and adequate capacity distribution for the lifecycle process (upgrades, patching). Take the number from Available Savings - this is the total amount of nodes you should have in the optimized state. headroom percentage = 100 / Amount_of_Nodes_in_suggested_optimized_state+1 In the Policies tab, it should look like this:","title":"Enable policies"},{"location":"guides/start-saving-quickly/#slow-and-safe-or-maximize-savings-now","text":"Evictor is our recommended way - it will constantly look for inefficiencies. But reducing costs in a safe manner takes time. If you want to maximize your savings as quickly as possible and you have a maintenance window, you can do it in CAST AI.","title":"\"Slow and safe\" or \"maximize savings now\""},{"location":"guides/start-saving-quickly/#install-evictor-continuous-improvements","text":"Evictor will compact your pods into fewer nodes, creating empty nodes that will be removed by the Node deletion policy: helm repo add castai https://castai.github.io/official-addons helm -n kube-system upgrade -i evictor castai/evictor --set dryRun=false This process will take some time. Also, Evictor will not cause any downtime to single replica deployments / StatefulSets, pods without ReplicaSet, meaning that those nodes can't be removed gracefully.","title":"Install Evictor (continuous improvements)"},{"location":"guides/start-saving-quickly/#stir-the-pod-with-manual-migration","text":"You will have to get rid of your existing nodes and let CAST AI create an optimized state right away. This might cause some downtime depending on your workload configuration. For example, pick 50% of your nodes in one availability zone (AZ) or 20% of nodes if your connected cluster is in a single AZ. kubectl get nodes -Lfailure-domain.beta.kubernetes.io/zone --selector=eks.amazonaws.com/nodegroup-image The percentage is arbitrary - it depends on your risk appetite and how much time you want to spend on this. Taint (cordon) the selected nodes, so no new pods are placed on these nodes. We like Lens k8s ide, but you can use kubectl as well: kubectl cordon nodeName1 kubectl cordon nodeName2 And now drain these nodes: kubectl drain nodeName1 --ignore-daemonsets --delete-local-data kubectl drain nodeName2 --ignore-daemonsets --delete-local-data Some nodes will not drain because of the Disruption Budget violation (downtime). These cases should be fixed since they are going to cause pain in the future (or at least noted to be addressed when most convenient). If you want to progress anyway and accept downtime, cancel the drain command and retry draining with the additional --force flag. You should see that the drained nodes disappear (empty Node deletion policy) and, in few moments, new nodes in the same availability zone appear (Unscheduled Pod policy with Headroom). Check the remaining nodes. You will see that list is shorter because the command below selects only nodes in the AWS autoscaling group (ASG) and new nodes don't use ASG. kubectl get nodes -Lfailure-domain.beta.kubernetes.io/zone --selector=eks.amazonaws.com/nodegroup-image Select next batch -> cordon -> drain -> write down problematic pod that don't migrate easily -> rinse and repeat until the list is empty.","title":"Stir the pod with manual migration"},{"location":"guides/start-saving-quickly/#utilize-spot-instances","text":"In the Available savings window, you can find a list of deployments that could use Spot instances. I have a recommendation service running with 10 replicas. I could separate this workload into two deployments: Reduce the current replica count to a bare minimum (in my case, 2 replicas), Create a copy of deployment with \"-spot\" appending name, add toleration, and set to 8 replicas - or beter, configure to use KEDA, see HPA documentation ... tolerations : - key : scheduling.cast.ai/spot operator : Exists ...","title":"Utilize Spot instances"},{"location":"guides/start-saving-quickly/#youre-all-done","text":"Share the Available savings window screenshot with your CFO/manager - there's nothing left to save. Reduce the Headroom policy to a smaller number that fits your smooth organic growth better. Install Evictor, if you haven't already done that.","title":"You're all done"},{"location":"guides/volumes/","text":"Dynamic volume provisioning \u00b6 Dynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to create new storage volumes manually (using cloud or storage providers) and the corresponding PersistentVolume objects for the storage to be available in Kubernetes. Dynamic volume provisioning is enabled by default on the CAST AI cluster. Overview \u00b6 Each CAST AI cluster is pre-configured with the default StorageClass that handles volume requests. \u00bb kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE cast-block-storage ( default ) storage.csi.cast.ai Delete WaitForFirstConsumer true 2m18s The binding mode WaitForFirstConsumer will delay the binding and provisioning of a PersistentVolume until a Pod using the PVC is created. Meaning, the volume will be created and attached to the Node on which a Pod using the PVC will be run. In the case of a Pod replicated across multiple clouds, volumes will be distributed across clouds as well. This will limit Pod scheduling only to the nodes of the same cloud since to reschedule a Pod to a different cloud service, the volume must be replicated to that cloud. This limitation will be removed by the cross-cloud volume replication feature which is not available at the moment. Deleting a cluster will delete all the volumes that were provisioned dynamically. Using dynamic volumes \u00b6 Creating persitent volume claim (PVC) \u00b6 Users can request dynamically provisioned storage by simply creating PersistentVolumeClaim and a Pod that will use it. apiVersion : v1 kind : PersistentVolumeClaim metadata : name : example-claim spec : accessModes : - ReadWriteOnce resources : requests : storage : 50Gi Pod example: apiVersion : v1 kind : Pod metadata : name : app spec : containers : - name : app image : centos command : [ \"/bin/sh\" ] args : [ \"-c\" , \"while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\" ] volumeMounts : - name : persistent-storage mountPath : /data volumes : - name : persistent-storage persistentVolumeClaim : claimName : example-claim This claim results in a Persistent Disk being automatically provisioned. When the claim is deleted, the volume is deleted as well. Volume claim templates \u00b6 Additionally, having StatefulSet user can define volumeClaimTemplates to provision volumes without creating PVC beforehand. apiVersion : v1 kind : Service metadata : name : nginx labels : app : nginx spec : ports : - port : 80 name : web clusterIP : None selector : app : nginx --- apiVersion : apps/v1 kind : StatefulSet metadata : name : web spec : selector : matchLabels : app : nginx serviceName : \"nginx\" replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : k8s.gcr.io/nginx-slim:0.8 ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/html volumeClaimTemplates : - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi This will result in dynamic PVC for each StatefulSet pod. \u00bb kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-3b550fd0-4b79-449d-b1cf-f51264f975fc 1Gi RWO cast-block-storage 3m11s www-web-1 Bound pvc-0c7470a8-cc59-49d4-b2ca-5d3db45c1b60 1Gi RWO cast-block-storage 2m41s www-web-2 Bound pvc-70c341cc-fa1c-471a-882a-e46225e1824f 1Gi RWO cast-block-storage 2m18s Deleting a StatefulSet will delete all provisioned volumes. Resizing PVC \u00b6 Any PVC created using cast-block-storage StorageClass can be edited to request more space. Kubernetes will interpret a change to the storage field as a request for more space. This will trigger automatic volume resizing. \u00bb kubectl edit pvc www-web-0 Change storage field as shown below: # www-web-0... spec : accessModes : - ReadWriteOnce resources : requests : storage : 10Gi # new storage size storageClassName : cast-block-storage # www-web-0... After storage is resized successfully, we can observe new PVC capacity: k get pvc www-web-0 NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-edd59e56-cb22-41b6-a075-ab8820f222b8 10Gi RWO cast-block-storage 4m57s","title":"Dynamic volume provisioning"},{"location":"guides/volumes/#dynamic-volume-provisioning","text":"Dynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to create new storage volumes manually (using cloud or storage providers) and the corresponding PersistentVolume objects for the storage to be available in Kubernetes. Dynamic volume provisioning is enabled by default on the CAST AI cluster.","title":"Dynamic volume provisioning"},{"location":"guides/volumes/#overview","text":"Each CAST AI cluster is pre-configured with the default StorageClass that handles volume requests. \u00bb kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE cast-block-storage ( default ) storage.csi.cast.ai Delete WaitForFirstConsumer true 2m18s The binding mode WaitForFirstConsumer will delay the binding and provisioning of a PersistentVolume until a Pod using the PVC is created. Meaning, the volume will be created and attached to the Node on which a Pod using the PVC will be run. In the case of a Pod replicated across multiple clouds, volumes will be distributed across clouds as well. This will limit Pod scheduling only to the nodes of the same cloud since to reschedule a Pod to a different cloud service, the volume must be replicated to that cloud. This limitation will be removed by the cross-cloud volume replication feature which is not available at the moment. Deleting a cluster will delete all the volumes that were provisioned dynamically.","title":"Overview"},{"location":"guides/volumes/#using-dynamic-volumes","text":"","title":"Using dynamic volumes"},{"location":"guides/volumes/#creating-persitent-volume-claim-pvc","text":"Users can request dynamically provisioned storage by simply creating PersistentVolumeClaim and a Pod that will use it. apiVersion : v1 kind : PersistentVolumeClaim metadata : name : example-claim spec : accessModes : - ReadWriteOnce resources : requests : storage : 50Gi Pod example: apiVersion : v1 kind : Pod metadata : name : app spec : containers : - name : app image : centos command : [ \"/bin/sh\" ] args : [ \"-c\" , \"while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\" ] volumeMounts : - name : persistent-storage mountPath : /data volumes : - name : persistent-storage persistentVolumeClaim : claimName : example-claim This claim results in a Persistent Disk being automatically provisioned. When the claim is deleted, the volume is deleted as well.","title":"Creating persitent volume claim (PVC)"},{"location":"guides/volumes/#volume-claim-templates","text":"Additionally, having StatefulSet user can define volumeClaimTemplates to provision volumes without creating PVC beforehand. apiVersion : v1 kind : Service metadata : name : nginx labels : app : nginx spec : ports : - port : 80 name : web clusterIP : None selector : app : nginx --- apiVersion : apps/v1 kind : StatefulSet metadata : name : web spec : selector : matchLabels : app : nginx serviceName : \"nginx\" replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : k8s.gcr.io/nginx-slim:0.8 ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/html volumeClaimTemplates : - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi This will result in dynamic PVC for each StatefulSet pod. \u00bb kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-3b550fd0-4b79-449d-b1cf-f51264f975fc 1Gi RWO cast-block-storage 3m11s www-web-1 Bound pvc-0c7470a8-cc59-49d4-b2ca-5d3db45c1b60 1Gi RWO cast-block-storage 2m41s www-web-2 Bound pvc-70c341cc-fa1c-471a-882a-e46225e1824f 1Gi RWO cast-block-storage 2m18s Deleting a StatefulSet will delete all provisioned volumes.","title":"Volume claim templates"},{"location":"guides/volumes/#resizing-pvc","text":"Any PVC created using cast-block-storage StorageClass can be edited to request more space. Kubernetes will interpret a change to the storage field as a request for more space. This will trigger automatic volume resizing. \u00bb kubectl edit pvc www-web-0 Change storage field as shown below: # www-web-0... spec : accessModes : - ReadWriteOnce resources : requests : storage : 10Gi # new storage size storageClassName : cast-block-storage # www-web-0... After storage is resized successfully, we can observe new PVC capacity: k get pvc www-web-0 NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-edd59e56-cb22-41b6-a075-ab8820f222b8 10Gi RWO cast-block-storage 4m57s","title":"Resizing PVC"},{"location":"guides/vpa/","text":"Right Sizing recommendations with VPA \u00b6 Recommendations example \u00b6 \u2190 On the left side you can see: The initial values that were set before Virtual Pod Autoscaler (VPA) took action. One of the pods is being terminated, due to auto mode changing the CPU/MEM values right away (causing potential downtime). \u2192 On the right side you can see: VPA recommendation couple minutes right after. VPA install guide \u00b6 In order for the instructions to work on macOS one needs to have latest version of OpenSSL from Homebrew: brew install openssl (make use of that OpenSSL version, rather than macOS native one). Run brew info openssl to see the instructions for setting this openssl version as your default one. Metrics-server up and running If you don't have metrics-server installed, run the following to install it: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml Run kubectl top nodes to check if metrics server was installed successfully Vertical Pod Autoscaler installed Prerequisite: helm Install helm chart helm repo add fairwinds-stable https://charts.fairwinds.com/stable helm install vpa fairwinds-stable/vpa --namespace vpa --create-namespace Run helm chart tests to make sure VPA is successfully installed helm test vpa -n vpa Output similar to this should be visible: LAST DEPLOYED: Thu Apr 29 19:53:50 2021 NAMESPACE: vpa STATUS: deployed REVISION: 1 TEST SUITE: vpa-test Last Started: Thu Apr 29 20:04:13 2021 Last Completed: Thu Apr 29 20:04:13 2021 Phase: Succeeded TEST SUITE: vpa-test Last Started: Thu Apr 29 20:04:14 2021 Last Completed: Thu Apr 29 20:04:14 2021 Phase: Succeeded TEST SUITE: vpa-test Last Started: Thu Apr 29 20:04:14 2021 Last Completed: Thu Apr 29 20:04:14 2021 Phase: Succeeded TEST SUITE: vpa-checkpoint-crd-available Last Started: Thu Apr 29 20:04:14 2021 Last Completed: Thu Apr 29 20:04:16 2021 Phase: Succeeded TEST SUITE: vpa-crd-available Last Started: Thu Apr 29 20:04:16 2021 Last Completed: Thu Apr 29 20:04:19 2021 Phase: Succeeded TEST SUITE: vpa-test-create-vpa Last Started: Thu Apr 29 20:04:21 2021 Last Completed: Thu Apr 29 20:04:49 2021 Phase: Succeeded TEST SUITE: vpa-metrics-api-available Last Started: Thu Apr 29 20:04:19 2021 Last Completed: Thu Apr 29 20:04:21 2021 Phase: Succeeded NOTES: Congratulations on installing the Vertical Pod Autoscaler! Components Installed: - recommender - updater Create VPA for each deployment to get the recommendations See Configure VPA for your deployment below for examples. Each deployment that wants make use of VPA, needs to have VPA created for it. Wait a day for the VPA. Send us the output of: kubectl get vpa -A -o yaml > recommendations.txt Troubleshooting \u00b6 In case something goes wrong, detailed instructions can be found at our github . Make sure that on macOS the openssl from Homebrew is used Configure VPA for your deployment \u00b6 In order to see how it works, you can use the example bellow, which would create both example deployment & example VPA configuration for that deployment. Once the deployment yaml is applied using kubectl view recommendations with the following line: kubectl describe vpa/hamster-vpa Example output: Status: Conditions: Last Transition Time: 2021-04-27T06:13:54Z Status: True Type: RecommendationProvided Recommendation: Container Recommendations: Container Name: hamster Lower Bound: Cpu: 491m Memory: 262144k Target: Cpu: 587m Memory: 262144k Uncapped Target: Cpu: 587m Memory: 262144k Upper Bound: Cpu: 1 Memory: 262144k Events: <none> Example deployment with VPA: # This config creates a deployment with two pods, each requesting 100 millicores # and trying to utilize slightly above 500 millicores (repeatedly using CPU for # 0.5s and sleeping 0.5s). # It also creates a corresponding Vertical Pod Autoscaler that adjusts the # requests. # Note that the update mode is left unset, so it defaults to \"Auto\" mode. --- apiVersion: \"autoscaling.k8s.io/v1\" kind: VerticalPodAutoscaler metadata: name: hamster-vpa namespace: default spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: hamster updatePolicy: # updateMode set to Off # - runs in recommendation mode # - does not mess with your pod request/limit configurations updateMode: \"Off\" resourcePolicy: containerPolicies: - containerName: '*' controlledResources: [\"cpu\", \"memory\"] --- apiVersion: apps/v1 kind: Deployment metadata: name: hamster namespace: default spec: selector: matchLabels: app: hamster replicas: 2 template: metadata: labels: app: hamster spec: securityContext: runAsNonRoot: true runAsUser: 65534 # nobody containers: - name: hamster image: k8s.gcr.io/ubuntu-slim:0.1 resources: requests: cpu: 100m memory: 50Mi command: [\"/bin/sh\"] args: - \"-c\" - \"while true; do timeout 0.5s yes >/dev/null; sleep 0.5s; done\" Available VPA modes \u00b6 \"Off\" : VPA does not automatically change resource requirements of the pods. The recommendations are calculated and can be inspected in the VPA object. \"Auto\" : VPA assigns resource requests on pod creation as well as updates them on existing pods using the preferred update mechanism. Currently this is equivalent to \"Recreate\" (see below). Once restart free (\"in-place\") update of pod requests is available, it may be used as the preferred update mechanism by the \"Auto\" mode. Warning \"Auto\" feature of VPA is experimental and may cause downtime for your applications. \"Initial\" : VPA only assigns resource requests on pod creation and never changes them later. Resources \u00b6 VPA definitive guide","title":"Right Sizing recommendations with VPA"},{"location":"guides/vpa/#right-sizing-recommendations-with-vpa","text":"","title":"Right Sizing recommendations with VPA"},{"location":"guides/vpa/#recommendations-example","text":"\u2190 On the left side you can see: The initial values that were set before Virtual Pod Autoscaler (VPA) took action. One of the pods is being terminated, due to auto mode changing the CPU/MEM values right away (causing potential downtime). \u2192 On the right side you can see: VPA recommendation couple minutes right after.","title":"Recommendations example"},{"location":"guides/vpa/#vpa-install-guide","text":"In order for the instructions to work on macOS one needs to have latest version of OpenSSL from Homebrew: brew install openssl (make use of that OpenSSL version, rather than macOS native one). Run brew info openssl to see the instructions for setting this openssl version as your default one. Metrics-server up and running If you don't have metrics-server installed, run the following to install it: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml Run kubectl top nodes to check if metrics server was installed successfully Vertical Pod Autoscaler installed Prerequisite: helm Install helm chart helm repo add fairwinds-stable https://charts.fairwinds.com/stable helm install vpa fairwinds-stable/vpa --namespace vpa --create-namespace Run helm chart tests to make sure VPA is successfully installed helm test vpa -n vpa Output similar to this should be visible: LAST DEPLOYED: Thu Apr 29 19:53:50 2021 NAMESPACE: vpa STATUS: deployed REVISION: 1 TEST SUITE: vpa-test Last Started: Thu Apr 29 20:04:13 2021 Last Completed: Thu Apr 29 20:04:13 2021 Phase: Succeeded TEST SUITE: vpa-test Last Started: Thu Apr 29 20:04:14 2021 Last Completed: Thu Apr 29 20:04:14 2021 Phase: Succeeded TEST SUITE: vpa-test Last Started: Thu Apr 29 20:04:14 2021 Last Completed: Thu Apr 29 20:04:14 2021 Phase: Succeeded TEST SUITE: vpa-checkpoint-crd-available Last Started: Thu Apr 29 20:04:14 2021 Last Completed: Thu Apr 29 20:04:16 2021 Phase: Succeeded TEST SUITE: vpa-crd-available Last Started: Thu Apr 29 20:04:16 2021 Last Completed: Thu Apr 29 20:04:19 2021 Phase: Succeeded TEST SUITE: vpa-test-create-vpa Last Started: Thu Apr 29 20:04:21 2021 Last Completed: Thu Apr 29 20:04:49 2021 Phase: Succeeded TEST SUITE: vpa-metrics-api-available Last Started: Thu Apr 29 20:04:19 2021 Last Completed: Thu Apr 29 20:04:21 2021 Phase: Succeeded NOTES: Congratulations on installing the Vertical Pod Autoscaler! Components Installed: - recommender - updater Create VPA for each deployment to get the recommendations See Configure VPA for your deployment below for examples. Each deployment that wants make use of VPA, needs to have VPA created for it. Wait a day for the VPA. Send us the output of: kubectl get vpa -A -o yaml > recommendations.txt","title":"VPA install guide"},{"location":"guides/vpa/#troubleshooting","text":"In case something goes wrong, detailed instructions can be found at our github . Make sure that on macOS the openssl from Homebrew is used","title":"Troubleshooting"},{"location":"guides/vpa/#configure-vpa-for-your-deployment","text":"In order to see how it works, you can use the example bellow, which would create both example deployment & example VPA configuration for that deployment. Once the deployment yaml is applied using kubectl view recommendations with the following line: kubectl describe vpa/hamster-vpa Example output: Status: Conditions: Last Transition Time: 2021-04-27T06:13:54Z Status: True Type: RecommendationProvided Recommendation: Container Recommendations: Container Name: hamster Lower Bound: Cpu: 491m Memory: 262144k Target: Cpu: 587m Memory: 262144k Uncapped Target: Cpu: 587m Memory: 262144k Upper Bound: Cpu: 1 Memory: 262144k Events: <none> Example deployment with VPA: # This config creates a deployment with two pods, each requesting 100 millicores # and trying to utilize slightly above 500 millicores (repeatedly using CPU for # 0.5s and sleeping 0.5s). # It also creates a corresponding Vertical Pod Autoscaler that adjusts the # requests. # Note that the update mode is left unset, so it defaults to \"Auto\" mode. --- apiVersion: \"autoscaling.k8s.io/v1\" kind: VerticalPodAutoscaler metadata: name: hamster-vpa namespace: default spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: hamster updatePolicy: # updateMode set to Off # - runs in recommendation mode # - does not mess with your pod request/limit configurations updateMode: \"Off\" resourcePolicy: containerPolicies: - containerName: '*' controlledResources: [\"cpu\", \"memory\"] --- apiVersion: apps/v1 kind: Deployment metadata: name: hamster namespace: default spec: selector: matchLabels: app: hamster replicas: 2 template: metadata: labels: app: hamster spec: securityContext: runAsNonRoot: true runAsUser: 65534 # nobody containers: - name: hamster image: k8s.gcr.io/ubuntu-slim:0.1 resources: requests: cpu: 100m memory: 50Mi command: [\"/bin/sh\"] args: - \"-c\" - \"while true; do timeout 0.5s yes >/dev/null; sleep 0.5s; done\"","title":"Configure VPA for your deployment"},{"location":"guides/vpa/#available-vpa-modes","text":"\"Off\" : VPA does not automatically change resource requirements of the pods. The recommendations are calculated and can be inspected in the VPA object. \"Auto\" : VPA assigns resource requests on pod creation as well as updates them on existing pods using the preferred update mechanism. Currently this is equivalent to \"Recreate\" (see below). Once restart free (\"in-place\") update of pod requests is available, it may be used as the preferred update mechanism by the \"Auto\" mode. Warning \"Auto\" feature of VPA is experimental and may cause downtime for your applications. \"Initial\" : VPA only assigns resource requests on pod creation and never changes them later.","title":"Available VPA modes"},{"location":"guides/vpa/#resources","text":"VPA definitive guide","title":"Resources"}]}